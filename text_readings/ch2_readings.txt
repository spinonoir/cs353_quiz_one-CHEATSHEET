CHAPTER

2
Application
Layer

Network applications are the raisons d’être of a computer network—if we couldn’t
conceive of any useful applications, there wouldn’t be any need for networking infrastructure and protocols to support them. Since the Internet’s inception, numerous useful
and entertaining applications have indeed been created. These applications have been the
driving force behind the Internet’s success, motivating people in homes, schools, governments, and businesses to make the Internet an integral part of their daily activities.
Internet applications include the classic text-based applications that became popular in the 1970s and 1980s: text e-mail, remote access to computers, file transfers, and
newsgroups. They include the killer application of the mid-1990s, the World Wide
Web, encompassing Web surfing, search, and electronic commerce. Since the beginning of new millennium, new and highly compelling applications continue to emerge,
including voice over IP and video conferencing such as Skype, Facetime, and Google
Hangouts; user generated video such as YouTube and movies on demand such as
Netflix; and multiplayer online games such as Second Life and World of Warcraft.
During this same period, we have seen the emergence of a new generation of social
networking applications—such as Facebook, Instagram, and Twitter—which have
created human networks on top of the Internet’s network or routers and communication links. And most recently, along with the arrival of the smartphone and the
ubiquity of 4G/5G wireless Internet access, there has been a profusion of location
based mobile apps, including popular check-in, dating, and road-traffic forecasting
apps (such as Yelp, Tinder, and Waz), mobile payment apps (such as WeChat and
Apple Pay) and messaging apps (such as WeChat and WhatsApp). Clearly, there has
been no slowing down of new and exciting Internet applications. Perhaps some of
the readers of this text will create the next generation of killer Internet applications!

81
81
81

82

CHAPTER 2

•

APPLICATION LAYER

In this chapter, we study the conceptual and implementation aspects of network
applications. We begin by defining key application-layer concepts, including network services required by applications, clients and servers, processes, and transport-layer interfaces. We examine several network applications in detail, including the
Web, e-mail, DNS, peer-to-peer (P2P) file distribution, and video streaming. We then
cover network application development, over both TCP and UDP. In particular, we
study the socket interface and walk through some simple client-server applications
in Python. We also provide several fun and interesting socket programming assignments at the end of the chapter.
The application layer is a particularly good place to start our study of protocols.
It’s familiar ground. We’re acquainted with many of the applications that rely on
the protocols we’ll study. It will give us a good feel for what protocols are all about
and will introduce us to many of the same issues that we’ll see again when we study
transport, network, and link layer protocols.

2.1 Principles of Network Applications
Suppose you have an idea for a new network application. Perhaps this application
will be a great service to humanity, or will please your professor, or will bring you
great wealth, or will simply be fun to develop. Whatever the motivation may be, let’s
now examine how you transform the idea into a real-world network application.
At the core of network application development is writing programs that run on
different end systems and communicate with each other over the network. For example, in the Web application there are two distinct programs that communicate with
each other: the browser program running in the user’s host (desktop, laptop, tablet,
smartphone, and so on); and the Web server program running in the Web server host.
As another example, in a Video on Demand application such as Netflix (see Section!2.6), there is a Netflix-provided program running on the user’s smartphone, tablet,
or computer; and a Netflix server program running on the Netflix server host. Servers
often (but certainly not always) are housed in a data center, as shown in Figure 2.1.
Thus, when developing your new application, you need to write software that
will run on multiple end systems. This software could be written, for example, in
C, Java, or Python. Importantly, you do not need to write software that runs on network-core devices, such as routers or link-layer switches. Even if you wanted to
write application software for these network-core devices, you wouldn’t be able to
do so. As we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core
devices do not function at the application layer but instead function at lower layers—
specifically at the network layer and below. This basic design—namely, confining
application software to the end systems—as shown in Figure 2.1, has facilitated the
rapid development and deployment of a vast array of network applications.

2.1

•

PRINCIPLES OF NETWORK APPLICATIONS

83

Application
Transport
Network
Data Link
Physical
National or
Global ISP

Application
Transport
Network
Data Link

Mobile Network

Physical
Datacenter Network

Datacenter Network

Home Network

Local or
Regional ISP

Content Provider Network

Application
Transport
Network
Data Link
Physical

Enterprise Network

Figure 2.1 ♦ Communication for a network application takes place
between end systems at the application layer

84

CHAPTER 2

•

APPLICATION LAYER

2.1.1 Network Application Architectures
Before diving into software coding, you should have a broad architectural plan for
your application. Keep in mind that an application’s architecture is distinctly different from the network architecture (e.g., the five-layer Internet architecture discussed
in Chapter 1). From the application developer’s perspective, the network architecture is fixed and provides a specific set of services to applications. The application
architecture, on the other hand, is designed by the application developer and dictates how the application is structured over the various end systems. In choosing
the application architecture, an application developer will likely draw on one of the
two predominant architectural paradigms used in modern network applications: the
client-server architecture or the peer-to-peer (P2P) architecture.
In a client-server architecture, there is an always-on host, called the server,
which services requests from many other hosts, called clients. A classic example
is the Web application for which an always-on Web server services requests from
browsers running on client hosts. When a Web server receives a request for an object
from a client host, it responds by sending the requested object to the client host. Note
that with the client-server architecture, clients do not directly communicate with each
other; for example, in the Web application, two browsers do not directly communicate. Another characteristic of the client-server architecture is that the server has a
fixed, well-known address, called an IP address (which we’ll discuss soon). Because
the server has a fixed, well-known address, and because the server is always on, a client can always contact the server by sending a packet to the server’s IP address. Some
of the better-known applications with a client-server architecture include the Web,
FTP, Telnet, and e-mail. The client-server architecture is shown in Figure 2.2(a).
Often in a client-server application, a single-server host is incapable of keeping up with all the requests from clients. For example, a popular social-networking
site can quickly become overwhelmed if it has only one server handling all of its
requests. For this reason, a data center, housing a large number of hosts, is often
used to create a powerful virtual server. The most popular Internet services—such
as search engines (e.g., Google, Bing, Baidu), Internet commerce (e.g., Amazon,
eBay, Alibaba), Web-based e-mail (e.g., Gmail and Yahoo Mail), social media (e.g.,
Facebook, Instagram, Twitter, and WeChat)—run in one or more data centers. As
discussed in Section 1.3.3, Google has 19 data centers distributed around the world,
which collectively handle search, YouTube, Gmail, and other services. A data center
can have hundreds of thousands of servers, which must be powered and maintained.
Additionally, the service providers must pay recurring interconnection and bandwidth costs for sending data from their data centers.
In a P2P architecture, there is minimal (or no) reliance on dedicated servers in
data centers. Instead the application exploits direct communication between pairs of
intermittently connected hosts, called peers. The peers are not owned by the service
provider, but are instead desktops and laptops controlled by users, with most of the
peers residing in homes, universities, and offices. Because the peers communicate

2.1

a. Client-server architecture

•

PRINCIPLES OF NETWORK APPLICATIONS

b. Peer-to-peer architecture

Figure 2.2 ♦ (a) Client-server architecture; (b) P2P architecture

without passing through a dedicated server, the architecture is called peer-to-peer.
An example of a popular P2P application is the file-sharing application BitTorrent.
One of the most compelling features of P2P architectures is their selfscalability. For example, in a P2P file-sharing application, although each peer
generates workload by requesting files, each peer also adds service capacity to the
system by distributing files to other peers. P2P architectures are also cost effective,
since they normally don’t require significant server infrastructure and server bandwidth (in contrast with clients-server designs with datacenters). However, P2P applications face challenges of security, performance, and reliability due to their highly
decentralized structure.

2.1.2 Processes Communicating
Before building your network application, you also need a basic understanding of
how the programs, running in multiple end systems, communicate with each other.
In the jargon of operating systems, it is not actually programs but processes that

85

86

CHAPTER 2

•

APPLICATION LAYER

communicate. A process can be thought of as a program that is running within an end
system. When processes are running on the same end system, they can communicate
with each other with interprocess communication, using rules that are governed by
the end system’s operating system. But in this book, we are not particularly interested
in how processes in the same host communicate, but instead in how processes running on different hosts (with potentially different operating systems) communicate.
Processes on two different end systems communicate with each other by
exchanging messages across the computer network. A sending process creates and
sends messages into the network; a receiving process receives these messages and
possibly responds by sending messages back. Figure 2.1 illustrates that processes
communicating with each other reside in the application layer of the five-layer protocol stack.

Client and Server Processes
A network application consists of pairs of processes that send messages to each
other over a network. For example, in the Web application a client browser process
exchanges messages with a Web server process. In a P2P file-sharing system, a file
is transferred from a process in one peer to a process in another peer. For each pair of
communicating processes, we typically label one of the two processes as the client
and the other process as the server. With the Web, a browser is a client process and
a Web server is a server process. With P2P file sharing, the peer that is downloading
the file is labeled as the client, and the peer that is uploading the file is labeled as
the server.
You may have observed that in some applications, such as in P2P file sharing,
a process can be both a client and a server. Indeed, a process in a P2P file-sharing
system can both upload and download files. Nevertheless, in the context of any given
communication session between a pair of processes, we can still label one process
as the client and the other process as the server. We define the client and server processes as follows:
In the context of a communication session between a pair of processes, the process that initiates the communication (that is, initially contacts the other process
at the beginning of the session) is labeled as the client. The process that waits to
be contacted to begin the session is the server.
In the Web, a browser process initializes contact with a Web server process;
hence the browser process is the client and the Web server process is the server. In
P2P file sharing, when Peer A asks Peer B to send a specific file, Peer A is the client and Peer B is the server in the context of this specific communication session.
When there’s no confusion, we’ll sometimes also use the terminology “client side
and server side of an application.” At the end of this chapter, we’ll step through simple code for both the client and server sides of network applications.

2.1

•

PRINCIPLES OF NETWORK APPLICATIONS

The Interface Between the Process and the Computer Network
As noted above, most applications consist of pairs of communicating processes, with
the two processes in each pair sending messages to each other. Any message sent
from one process to another must go through the underlying network. A process
sends messages into, and receives messages from, the network through a software
interface called a socket. Let’s consider an analogy to help us understand processes
and sockets. A process is analogous to a house and its socket is analogous to its door.
When a process wants to send a message to another process on another host, it shoves
the message out its door (socket). This sending process assumes that there is a transportation infrastructure on the other side of its door that will transport the message to
the door of the destination process. Once the message arrives at the destination host,
the message passes through the receiving process’s door (socket), and the receiving
process then acts on the message.
Figure 2.3 illustrates socket communication between two processes that communicate over the Internet. (Figure 2.3 assumes that the underlying transport protocol
used by the processes is the Internet’s TCP protocol.) As shown in this figure, a socket
is the interface between the application layer and the transport layer within a host. It
is also referred to as the Application Programming Interface (API) between the
application and the network, since the socket is the programming interface with which
network applications are built. The application developer has control of everything on
the application-layer side of the socket but has little control of the transport-layer side
of the socket. The only control that the application developer has on the transportlayer side is (1) the choice of transport protocol and (2) perhaps the ability to fix a few

Controlled
by application
developer

Controlled
by operating
system

Host or
server

Host or
server

Process

Process

Socket

Socket

TCP with
buffers,
variables

TCP with
buffers,
variables

Internet

Figure 2.3 ♦ Application processes, sockets, and underlying transport protocol

Controlled
by application
developer

Controlled
by operating
system

87

88

CHAPTER 2

•

APPLICATION LAYER

transport-layer parameters such as maximum buffer and maximum segment sizes (to
be covered in Chapter 3). Once the application developer chooses a transport protocol
(if a choice is available), the application is built using the transport-layer services
provided by that protocol. We’ll explore sockets in some detail in Section 2.7.

Addressing Processes
In order to send postal mail to a particular destination, the destination needs to have
an address. Similarly, in order for a process running on one host to send packets to
a process running on another host, the receiving process needs to have an address.
To identify the receiving process, two pieces of information need to be specified:
(1)!the address of the host and (2) an identifier that specifies the receiving process in
the destination host.
In the Internet, the host is identified by its IP address. We’ll discuss IP addresses
in great detail in Chapter 4. For now, all we need to know is that an IP address is a 32-bit
quantity that we can think of as uniquely identifying the host. In addition to knowing the address of the host to which a message is destined, the sending process must
also identify the receiving process (more specifically, the receiving socket) running in
the host. This information is needed because in general a host could be running many
network applications. A destination port number serves this purpose. Popular applications have been assigned specific port numbers. For example, a Web server is identified
by port number 80. A mail server process (using the SMTP protocol) is identified by
port number 25. A list of well-known port numbers for all Internet standard protocols
can be found at www.iana.org. We’ll examine port numbers in detail in Chapter 3.

2.1.3 Transport Services Available to Applications
Recall that a socket is the interface between the application process and the transport-layer protocol. The application at the sending side pushes messages through the
socket. At the other side of the socket, the transport-layer protocol has the responsibility of getting the messages to the socket of the receiving process.
Many networks, including the Internet, provide more than one transport-layer
protocol. When you develop an application, you must choose one of the available
transport-layer protocols. How do you make this choice? Most likely, you would
study the services provided by the available transport-layer protocols, and then pick
the protocol with the services that best match your application’s needs. The situation
is similar to choosing either train or airplane transport for travel between two cities.
You have to choose one or the other, and each transportation mode offers different
services. (For example, the train offers downtown pickup and drop-off, whereas the
plane offers shorter travel time.)
What are the services that a transport-layer protocol can offer to applications
invoking it? We can broadly classify the possible services along four dimensions:
reliable data transfer, throughput, timing, and security.

2.1

•

PRINCIPLES OF NETWORK APPLICATIONS

Reliable Data Transfer
As discussed in Chapter 1, packets can get lost within a computer network. For example, a packet can overflow a buffer in a router, or can be discarded by a host or router
after having some of its bits corrupted. For many applications—such as electronic
mail, file transfer, remote host access, Web document transfers, and financial applications—data loss can have devastating consequences (in the latter case, for either
the bank or the customer!). Thus, to support these applications, something has to be
done to guarantee that the data sent by one end of the application is delivered correctly and completely to the other end of the application. If a protocol provides such
a guaranteed data delivery service, it is said to provide reliable data transfer. One
important service that a transport-layer protocol can potentially provide to an application is process-to-process reliable data transfer. When a transport protocol provides
this service, the sending process can just pass its data into the socket and know with
complete confidence that the data will arrive without errors at the receiving process.
When a transport-layer protocol doesn’t provide reliable data transfer, some of
the data sent by the sending process may never arrive at the receiving process. This
may be acceptable for loss-tolerant applications, most notably multimedia applications such as conversational audio/video that can tolerate some amount of data loss.
In these multimedia applications, lost data might result in a small glitch in the audio/
video—not a crucial impairment.

Throughput
In Chapter 1, we introduced the concept of available throughput, which, in the
context of a communication session between two processes along a network path,
is the rate at which the sending process can deliver bits to the receiving process.
Because other sessions will be sharing the bandwidth along the network path, and
because these other sessions will be coming and going, the available throughput
can fluctuate with time. These observations lead to another natural service that a
transport-layer protocol could provide, namely, guaranteed available throughput at
some specified rate. With such a service, the application could request a guaranteed
throughput of r bits/sec, and the transport protocol would then ensure that the available throughput is always at least r bits/sec. Such a guaranteed throughput service
would appeal to many applications. For example, if an Internet telephony application encodes voice at 32 kbps, it needs to send data into the network and have data
delivered to the receiving application at this rate. If the transport protocol cannot
provide this throughput, the application would need to encode at a lower rate (and
receive enough throughput to sustain this lower coding rate) or may have to give
up, since receiving, say, half of the needed throughput is of little or no use to this
Internet telephony application. Applications that have throughput requirements are
said to be bandwidth-sensitive applications. Many current multimedia applications
are bandwidth sensitive, although some multimedia applications may use adaptive

89

90

CHAPTER 2

•

APPLICATION LAYER

coding techniques to encode digitized voice or video at a rate that matches the currently available throughput.
While bandwidth-sensitive applications have specific throughput requirements,
elastic applications can make use of as much, or as little, throughput as happens to
be available. Electronic mail, file transfer, and Web transfers are all elastic applications. Of course, the more throughput, the better. There’s an adage that says that one
cannot be too rich, too thin, or have too much throughput!

Timing
A transport-layer protocol can also provide timing guarantees. As with throughput
guarantees, timing guarantees can come in many shapes and forms. An example
guarantee might be that every bit that the sender pumps into the socket arrives
at the receiver’s socket no more than 100 msec later. Such a service would be
appealing to interactive real-time applications, such as Internet telephony, virtual
environments, teleconferencing, and multiplayer games, all of which require tight
timing constraints on data delivery in order to be effective, see [Gauthier 1999;
Ramjee 1994]. Long delays in Internet telephony, for example, tend to result in
unnatural pauses in the conversation; in a multiplayer game or virtual interactive
environment, a long delay between taking an action and seeing the response from
the environment (for example, from another player at the end of an end-to-end connection) makes the application feel less realistic. For non-real-time applications,
lower delay is always preferable to higher delay, but no tight constraint is placed
on the end-to-end delays.

Security
Finally, a transport protocol can provide an application with one or more security
services. For example, in the sending host, a transport protocol can encrypt all data
transmitted by the sending process, and in the receiving host, the transport-layer protocol can decrypt the data before delivering the data to the receiving process. Such a
service would provide confidentiality between the two processes, even if the data is
somehow observed between sending and receiving processes. A transport protocol
can also provide other security services in addition to confidentiality, including data
integrity and end-point authentication, topics that we’ll cover in detail in Chapter 8.

2.1.4 Transport Services Provided by the Internet
Up until this point, we have been considering transport services that a computer network could provide in general. Let’s now get more specific and examine the type of
transport services provided by the Internet. The Internet (and, more generally, TCP/
IP networks) makes two transport protocols available to applications, UDP and TCP.
When you (as an application developer) create a new network application for the

2.1

•

PRINCIPLES OF NETWORK APPLICATIONS

Application

Data Loss

Throughput

Time-Sensitive

File transfer/download

No loss

Elastic

No

E-mail

No loss

Elastic

No

Web documents

No loss

Elastic (few kbps)

No

Internet telephony/
Video conferencing

Loss-tolerant

Audio: few kbps–1 Mbps
Video: 10 kbps–5 Mbps

Yes: 100s of msec

Streaming stored
audio/video

Loss-tolerant

Same as above

Yes: few seconds

Interactive games

Loss-tolerant

Few kbps–10 kbps

Yes: 100s of msec

Smartphone messaging

No loss

Elastic

Yes and no

Figure 2.4 ♦ Requirements of selected network applications

Internet, one of the first decisions you have to make is whether to use UDP or TCP.
Each of these protocols offers a different set of services to the invoking applications.
Figure 2.4 shows the service requirements for some selected applications.

TCP Services
The TCP service model includes a connection-oriented service and a reliable data
transfer service. When an application invokes TCP as its transport protocol, the
application receives both of these services from TCP.
• Connection-oriented service. TCP has the client and server exchange transportlayer control information with each other before the application-level messages begin to flow. This so-called handshaking procedure alerts the client
and server, allowing them to prepare for an onslaught of packets. After the
handshaking phase, a TCP connection is said to exist between the sockets
of the two processes. The connection is a full-duplex connection in that the two
processes can send messages to each other over the connection at the same time.
When the application finishes sending messages, it must tear down the connection. In Chapter 3, we’ll discuss connection-oriented service in detail and examine
how it is implemented.
• Reliable data transfer service. The communicating processes can rely on TCP to
deliver all data sent without error and in the proper order. When one side of the
application passes a stream of bytes into a socket, it can count on TCP to deliver the
same stream of bytes to the receiving socket, with no missing or duplicate bytes.

91

92

CHAPTER 2

•

APPLICATION LAYER

TCP also includes a congestion-control mechanism, a service for the general
welfare of the Internet rather than for the direct benefit of the communicating processes. The TCP congestion-control mechanism throttles a sending process (client or
server) when the network is congested between sender and receiver. As we will see
in Chapter 3, TCP congestion control also attempts to limit each TCP connection to
its fair share of network bandwidth.

UDP Services
UDP is a no-frills, lightweight transport protocol, providing minimal services. UDP
is connectionless, so there is no handshaking before the two processes start to communicate. UDP provides an unreliable data transfer service—that is, when a process
sends a message into a UDP socket, UDP provides no guarantee that the message
will ever reach the receiving process. Furthermore, messages that do arrive at the
receiving process may arrive out of order.

FOCUS ON SECURITY
SECURING T CP
Neither TCP nor UDP provides any encryption—the data that the sending process
passes into its socket is the same data that travels over the network to the destination process. So, for example, if the sending process sends a password in cleartext
(i.e., unencrypted) into its socket, the cleartext password will travel over all the links
between sender and receiver, potentially getting sniffed and discovered at any of the
intervening links. Because privacy and other security issues have become critical for
many applications, the Internet community has developed an enhancement for TCP,
called Transport Layer Security (TLS) [RFC 5246]. TCP-enhanced-with-TLS not
only does everything that traditional TCP does but also provides critical process-toprocess security services, including encryption, data integrity, and end-point authentication. We emphasize that TLS is not a third Internet transport protocol, on the same
level as TCP and UDP, but instead is an enhancement of TCP, with the enhancements
being implemented in the application layer. In particular, if an application wants to
use the services of TLS, it needs to include TLS code (existing, highly optimized libraries and classes) in both the client and server sides of the application. TLS has its own
socket API that is similar to the traditional TCP socket API. When an application uses
TLS, the sending process passes cleartext data to the TLS socket; TLS in the sending
host then encrypts the data and passes the encrypted data to the TCP socket. The
encrypted data travels over the Internet to the TCP socket in the receiving process.
The receiving socket passes the encrypted data to TLS, which decrypts the data.
Finally, TLS passes the cleartext data through its TLS socket to the receiving process.
We’ll cover TLS in some detail in Chapter 8.

2.1

•

PRINCIPLES OF NETWORK APPLICATIONS

UDP does not include a congestion-control mechanism, so the sending side of
UDP can pump data into the layer below (the network layer) at any rate it pleases.
(Note, however, that the actual end-to-end throughput may be less than this rate due
to the limited transmission capacity of intervening links or due to congestion).

Services Not Provided by Internet Transport Protocols
We have organized transport protocol services along four dimensions: reliable data
transfer, throughput, timing, and security. Which of these services are provided by
TCP and UDP? We have already noted that TCP provides reliable end-to-end data
transfer. And we also know that TCP can be easily enhanced at the application
layer with TLS to provide security services. But in our brief description of TCP and
UDP, conspicuously missing was any mention of throughput or timing guarantees—
services not provided by today’s Internet transport protocols. Does this mean that timesensitive applications such as Internet telephony cannot run in today’s Internet? The
answer is clearly no—the Internet has been hosting time-sensitive applications for
many years. These applications often work fairly well because they have been designed
to cope, to the greatest extent possible, with this lack of guarantee. Nevertheless, clever
design has its limitations when delay is excessive, or the end-to-end throughput is
limited. In summary, today’s Internet can often provide satisfactory service to timesensitive applications, but it cannot provide any timing or throughput guarantees.
Figure 2.5 indicates the transport protocols used by some popular Internet applications. We see that e-mail, remote terminal access, the Web, and file transfer all use
TCP. These applications have chosen TCP primarily because TCP provides reliable
data transfer, guaranteeing that all data will eventually get to its destination. Because
Internet telephony applications (such as Skype) can often tolerate some loss but
require a minimal rate to be effective, developers of Internet telephony applications
Application

Application-Layer Protocol

Underlying Transport Protocol

Electronic mail

SMTP [RFC 5321]

TCP

Remote terminal access

Telnet [RFC 854]

TCP

Web

HTTP 1.1 [RFC 7230]

TCP

File transfer

FTP [RFC 959]

TCP

Streaming multimedia

HTTP (e.g., YouTube), DASH

TCP

Internet telephony

SIP [RFC 3261], RTP [RFC 3550], or proprietary
(e.g., Skype)

UDP or TCP

Figure 2.5 ♦ Popular Internet applications, their application-layer
protocols, and their underlying transport protocols

93

94

CHAPTER 2

•

APPLICATION LAYER

usually prefer to run their applications over UDP, thereby circumventing TCP’s
congestion control mechanism and packet overheads. But because many firewalls
are configured to block (most types of) UDP traffic, Internet telephony applications
often are designed to use TCP as a backup if UDP communication fails.

2.1.5 Application-Layer Protocols
We have just learned that network processes communicate with each other by sending
messages into sockets. But how are these messages structured? What are the meanings
of the various fields in the messages? When do the processes send the messages? These
questions bring us into the realm of application-layer protocols. An application-layer
protocol defines how an application’s processes, running on different end systems,
pass messages to each other. In particular, an application-layer protocol defines:
• The types of messages exchanged, for example, request messages and response
messages
• The syntax of the various message types, such as the fields in the message and
how the fields are delineated
• The semantics of the fields, that is, the meaning of the information in the fields
• Rules for determining when and how a process sends messages and responds to
messages
Some application-layer protocols are specified in RFCs and are therefore in the
public domain. For example, the Web’s application-layer protocol, HTTP (the
HyperText Transfer Protocol [RFC 7230]), is available as an RFC. If a browser
developer follows the rules of the HTTP RFC, the browser will be able to retrieve
Web pages from any Web server that has also followed the rules of the HTTP RFC.
Many other application-layer protocols are proprietary and intentionally not available in the public domain. For example, Skype uses proprietary application-layer
protocols.
It is important to distinguish between network applications and application-layer
protocols. An application-layer protocol is only one piece of a network application
(albeit, a very important piece of the application from our point of view!). Let’s look
at a couple of examples. The Web is a client-server application that allows users to
obtain documents from Web servers on demand. The Web application consists of
many components, including a standard for document formats (that is, HTML), Web
browsers (for example, Chrome and Microsoft Internet Explorer), Web servers
(for example, Apache and Microsoft servers), and an application-layer protocol.
The Web’s application-layer protocol, HTTP, defines the format and sequence
of messages exchanged between browser and Web server. Thus, HTTP is only
one piece (albeit, an important piece) of the Web application. As another example,
we’ll see in Section 2.6 that Netflix’s video service also has many components,

2.2

•

THE WEB AND HTTP

including servers that store and transmit videos, other servers that manage billing
and other client functions, clients (e.g., the Netflix app on your smartphone, tablet, or
computer), and an application-level DASH protocol defines the format and sequence
of messages exchanged between a Netflix server and client. Thus, DASH is only one
piece (albeit, an important piece) of the Netflix application.

2.1.6 Network Applications Covered in This Book
New applications are being developed every day. Rather than covering a large
number of Internet applications in an encyclopedic manner, we have chosen to
focus on a small number of applications that are both pervasive and important.
In this chapter, we discuss five important applications: the Web, electronic mail,
directory service, video streaming, and P2P applications. We first discuss the
Web, not only because it is an enormously popular application, but also because
its application-layer protocol, HTTP, is straightforward and easy to understand.
We then discuss electronic mail, the Internet’s first killer application. E-mail is
more complex than the Web in the sense that it makes use of not one but several application-layer protocols. After e-mail, we cover DNS, which provides a
directory service for the Internet. Most users do not interact with DNS directly;
instead, users invoke DNS indirectly through other applications (including the
Web, file transfer, and electronic mail). DNS illustrates nicely how a piece of
core network functionality (network-name to network-address translation) can
be implemented at the application layer in the Internet. We then discuss P2P file
sharing applications, and complete our application study by discussing video
streaming on demand, including distributing stored video over content distribution networks.

2.2 The Web and HTTP
Until the early 1990s, the Internet was used primarily by researchers, academics,
and university students to log in to remote hosts, to transfer files from local hosts to
remote hosts and vice versa, to receive and send news, and to receive and send electronic mail. Although these applications were (and continue to be) extremely useful,
the Internet was essentially unknown outside of the academic and research communities. Then, in the early 1990s, a major new application arrived on the scene—the
World Wide Web [Berners-Lee 1994]. The Web was the first Internet application
that caught the general public’s eye. It dramatically changed how people interact
inside and outside their work environments. It elevated the Internet from just one of
many data networks to essentially the one and only data network.
Perhaps what appeals the most to users is that the Web operates on demand.
Users receive what they want, when they want it. This is unlike traditional broadcast

95

96

CHAPTER 2

•

APPLICATION LAYER

radio and television, which force users to tune in when the content provider makes
the content available. In addition to being available on demand, the Web has many
other wonderful features that people love and cherish. It is enormously easy for any
individual to make information available over the Web—everyone can become a
publisher at extremely low cost. Hyperlinks and search engines help us navigate
through an ocean of information. Photos and videos stimulate our senses. Forms,
JavaScript, video, and many other devices enable us to interact with pages and sites.
And the Web and its protocols serve as a platform for YouTube, Web-based e-mail
(such as Gmail), and most mobile Internet applications, including Instagram and
Google Maps.

2.2.1 Overview of HTTP
The HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol,
is at the heart of the Web. It is defined in [RFC 1945], [RFC 7230] and [RFC 7540].
HTTP is implemented in two programs: a client program and a server program. The
client program and server program, executing on different end systems, talk to each
other by exchanging HTTP messages. HTTP defines the structure of these messages
and how the client and server exchange the messages. Before explaining HTTP in
detail, we should review some Web terminology.
A Web page (also called a document) consists of objects. An object is
simply a file—such as an HTML file, a JPEG image, a Javascrpt file, a CCS
style sheet file, or a video clip—that is addressable by a single URL. Most Web
pages consist of a base HTML file and several referenced objects. For example,
if a Web page contains HTML text and five JPEG images, then the Web page has
six objects: the base HTML file plus the five images. The base HTML file references the other objects in the page with the objects’ URLs. Each URL has two
components: the hostname of the server that houses the object and the object’s
path name. For example, the URL
http://www.someSchool.edu/someDepartment/picture.gif
has www.someSchool.edu for a hostname and /someDepartment/picture.
gif for a path name. Because Web browsers (such as Internet Explorer and Chrome)
implement the client side of HTTP, in the context of the Web, we will use the words
browser and client interchangeably. Web servers, which implement the server side
of HTTP, house Web objects, each addressable by a URL. Popular Web servers
include Apache and Microsoft Internet Information Server.
HTTP defines how Web clients request Web pages from Web servers and how
servers transfer Web pages to clients. We discuss the interaction between client
and server in detail later, but the general idea is illustrated in Figure 2.6. When a
user requests a Web page (for example, clicks on a hyperlink), the browser sends

2.2

•

THE WEB AND HTTP

Server running
Apache Web server

HT

TP

q
re
H

s
ue

P
TT

PC running
Internet Explorer

HT

t

re

o
sp

e
ns

HT

TP

TP

re

re

qu

sp

es

on

se

t

Android smartphone
running Google Chrome

Figure 2.6 ♦ HTTP request-response behavior

HTTP request messages for the objects in the page to the server. The server receives
the requests and responds with HTTP response messages that contain the objects.
HTTP uses TCP as its underlying transport protocol (rather than running on top
of UDP). The HTTP client first initiates a TCP connection with the server. Once the
connection is established, the browser and the server processes access TCP through
their socket interfaces. As described in Section 2.1, on the client side the socket interface is the door between the client process and the TCP connection; on the server side
it is the door between the server process and the TCP connection. The client sends
HTTP request messages into its socket interface and receives HTTP response messages from its socket interface. Similarly, the HTTP server receives request messages
from its socket interface and sends response messages into its socket interface. Once
the client sends a message into its socket interface, the message is out of the client’s
hands and is “in the hands” of TCP. Recall from Section 2.1 that TCP provides a
reliable data transfer service to HTTP. This implies that each HTTP request message
sent by a client process eventually arrives intact at the server; similarly, each HTTP
response message sent by the server process eventually arrives intact at the client.
Here we see one of the great advantages of a layered architecture—HTTP need not
worry about lost data or the details of how TCP recovers from loss or reordering of
data within the network. That is the job of TCP and the protocols in the lower layers
of the protocol stack.
It is important to note that the server sends requested files to clients without
storing any state information about the client. If a particular client asks for the same
object twice in a period of a few seconds, the server does not respond by saying
that it just served the object to the client; instead, the server resends the object, as
it has completely forgotten what it did earlier. Because an HTTP server maintains

97

98

CHAPTER 2

•

APPLICATION LAYER

no information about the clients, HTTP is said to be a stateless protocol. We also
remark that the Web uses the client-server application architecture, as described in
Section 2.1. A Web server is always on, with a fixed IP address, and it services
requests from potentially millions of different browsers.
The original version of HTTP is called HTTP/1.0 and dates back to the early
1990’s [RFC 1945]. As of 2020, the majority of HTTP transactions take place over
HTTP/1.1 [RFC 7230]. However, increasingly browsers and Web servers also support a new version of HTTP called HTTP/2 [RFC 7540]. At the end of this section,
we’ll provide an introduction to HTTP/2.

2.2.2 Non-Persistent and Persistent Connections
In many Internet applications, the client and server communicate for an extended
period of time, with the client making a series of requests and the server responding to each of the requests. Depending on the application and on how the
application is being used, the series of requests may be made back-to-back, periodically at regular intervals, or intermittently. When this client-server interaction
is taking place over TCP, the application developer needs to make an important
decision—should each request/response pair be sent over a separate TCP connection, or should all of the requests and their corresponding responses be sent over
the same TCP connection? In the former approach, the application is said to use
non-persistent connections; and in the latter approach, persistent connections. To
gain a deep understanding of this design issue, let’s examine the advantages and disadvantages of persistent connections in the context of a specific application, namely,
HTTP, which can use both non-persistent connections and persistent connections.
Although HTTP uses persistent connections in its default mode, HTTP clients and
servers can be configured to use non-persistent connections instead.

HTTP with Non-Persistent Connections
Let’s walk through the steps of transferring a Web page from server to client for the
case of non-persistent connections. Let’s suppose the page consists of a base HTML
file and 10 JPEG images, and that all 11 of these objects reside on the same server.
Further suppose the URL for the base HTML file is
http://www.someSchool.edu/someDepartment/home.index
Here is what happens:
1. The HTTP client process initiates a TCP connection to the server
www.someSchool.edu on port number 80, which is the default port number
for HTTP. Associated with the TCP connection, there will be a socket at the
client and a socket at the server.

2.2

•

THE WEB AND HTTP

2. The HTTP client sends an HTTP request message to the server via its socket.
The request message includes the path name /someDepartment/home
.index. (We will discuss HTTP messages in some detail below.)
3. The HTTP server process receives the request message via its socket, retrieves
the object /someDepartment/home.index from its storage (RAM or
disk), encapsulates the object in an HTTP response message, and sends the
response message to the client via its socket.
4. The HTTP server process tells TCP to close the TCP connection. (But TCP
doesn’t actually terminate the connection until it knows for sure that the client
has received the response message intact.)
5. The HTTP client receives the response message. The TCP connection terminates. The message indicates that the encapsulated object is an HTML file. The
client extracts the file from the response message, examines the HTML file, and
finds references to the 10 JPEG objects.
6. The first four steps are then repeated for each of the referenced JPEG objects.
As the browser receives the Web page, it displays the page to the user. Two
different browsers may interpret (that is, display to the user) a Web page in somewhat different ways. HTTP has nothing to do with how a Web page is interpreted
by a client. The HTTP specifications ([RFC 1945] and [RFC 7540]) define only the
communication protocol between the client HTTP program and the server HTTP
program.
The steps above illustrate the use of non-persistent connections, where each
TCP connection is closed after the server sends the object—the connection does not
persist for other objects. HTTP/1.0 employes non-persistent TCP connections. Note
that each non-persistent TCP connection transports exactly one request message and
one response message. Thus, in this example, when a user requests the Web page, 11
TCP connections are generated.
In the steps described above, we were intentionally vague about whether the
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the
JPEGs are obtained over parallel TCP connections. Indeed, users can configure some
browsers to control the degree of parallelism. Browsers may open multiple TCP connections and request different parts of the Web page over the multiple connections. As
we’ll see in the next chapter, the use of parallel connections shortens the response time.
Before continuing, let’s do a back-of-the-envelope calculation to estimate the
amount of time that elapses from when a client requests the base HTML file until
the entire file is received by the client. To this end, we define the round-trip time
(RTT), which is the time it takes for a small packet to travel from client to server
and then back to the client. The RTT includes packet-propagation delays, packetqueuing delays in intermediate routers and switches, and packet-processing delays.
(These delays were discussed in Section 1.4.) Now consider what happens when
a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser to
initiate a TCP connection between the browser and the Web server; this involves

99

100

CHAPTER 2

•

APPLICATION LAYER

Initiate TCP
connection

RTT

Request file

RTT

Time to transmit file

Entire file received

Time
at client

Time
at server

Figure 2.7 ♦ Back-of-the-envelope calculation for the time needed
to request and receive an HTML file
a “three-way handshake”—the client sends a small TCP segment to the server, the
server acknowledges and responds with a small TCP segment, and, finally, the client acknowledges back to the server. The first two parts of the three-way handshake
take one RTT. After completing the first two parts of the handshake, the client sends
the HTTP request message combined with the third part of the three-way handshake
(the acknowledgment) into the TCP connection. Once the request message arrives at
the server, the server sends the HTML file into the TCP connection. This HTTP
request/response eats up another RTT. Thus, roughly, the total response time is two
RTTs plus the transmission time at the server of the HTML file.

HTTP with Persistent Connections
Non-persistent connections have some shortcomings. First, a brand-new connection
must be established and maintained for each requested object. For each of these
connections, TCP buffers must be allocated and TCP variables must be kept in both
the client and server. This can place a significant burden on the Web server, which
may be serving requests from hundreds of different clients simultaneously. Second,

2.2

•

THE WEB AND HTTP

as we just described, each object suffers a delivery delay of two RTTs—one RTT to
establish the TCP connection and one RTT to request and receive an object.
With HTTP/1.1 persistent connections, the server leaves the TCP connection
open after sending a response. Subsequent requests and responses between the same
client and server can be sent over the same connection. In particular, an entire Web
page (in the example above, the base HTML file and the 10 images) can be sent over
a single persistent TCP connection. Moreover, multiple Web pages residing on the
same server can be sent from the server to the same client over a single persistent
TCP connection. These requests for objects can be made back-to-back, without waiting for replies to pending requests (pipelining). Typically, the HTTP server closes
a connection when it isn’t used for a certain time (a configurable timeout interval).
When the server receives the back-to-back requests, it sends the objects back-toback. The default mode of HTTP uses persistent connections with pipelining. We’ll
quantitatively compare the performance of non-persistent and persistent connections
in the homework problems of Chapters 2 and 3. You are also encouraged to see
[Heidemann 1997; Nielsen 1997; RFC 7540].

2.2.3 HTTP Message Format
The HTTP specifications [RFC 1945; RFC 7230; RFC 7540] include the definitions
of the HTTP message formats. There are two types of HTTP messages, request messages and response messages, both of which are discussed below.

HTTP Request Message
Below we provide a typical HTTP request message:
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
We can learn a lot by taking a close look at this simple request message. First of
all, we see that the message is written in ordinary ASCII text, so that your ordinary
computer-literate human being can read it. Second, we see that the message consists
of five lines, each followed by a carriage return and a line feed. The last line is followed by an additional carriage return and line feed. Although this particular request
message has five lines, a request message can have many more lines or as few as
one line. The first line of an HTTP request message is called the request line; the
subsequent lines are called the header lines. The request line has three fields: the
method field, the URL field, and the HTTP version field. The method field can take
on several different values, including GET, POST, HEAD, PUT, and DELETE.

101

102

CHAPTER 2

•

APPLICATION LAYER

The great majority of HTTP request messages use the GET method. The GET method
is used when the browser requests an object, with the requested object identified in
the URL field. In this example, the browser is requesting the object /somedir/
page.html. The version is self-explanatory; in this example, the browser implements version HTTP/1.1.
Now let’s look at the header lines in the example. The header line Host:
www.someschool.edu specifies the host on which the object resides. You might
think that this header line is unnecessary, as there is already a TCP connection in
place to the host. But, as we’ll see in Section 2.2.5, the information provided by the
host header line is required by Web proxy caches. By including the Connection:
close header line, the browser is telling the server that it doesn’t want to bother
with persistent connections; it wants the server to close the connection after sending
the requested object. The User-agent: header line specifies the user agent, that
is, the browser type that is making the request to the server. Here the user agent is
Mozilla/5.0, a Firefox browser. This header line is useful because the server can actually send different versions of the same object to different types of user agents. (Each
of the versions is addressed by the same URL.) Finally, the Accept-language:
header indicates that the user prefers to receive a French version of the object, if such
an object exists on the server; otherwise, the server should send its default version.
The Accept-language: header is just one of many content negotiation headers
available in HTTP.
Having looked at an example, let’s now look at the general format of a request
message, as shown in Figure 2.8. We see that the general format closely follows our
earlier example. You may have noticed, however, that after the header lines (and the
additional carriage return and line feed) there is an “entity body.” The entity body

Request line

method

sp

URL

sp

Version

header field name: sp

value

cr lf

header field name: sp

value

cr lf

Header lines

Blank line

cr lf

Entity body

Figure 2.8 ♦ General format of an HTTP request message

cr lf

2.2

•

THE WEB AND HTTP

is empty with the GET method, but is used with the POST method. An HTTP client
often uses the POST method when the user fills out a form—for example, when a
user provides search words to a search engine. With a POST message, the user is still
requesting a Web page from the server, but the specific contents of the Web page
depend on what the user entered into the form fields. If the value of the method field
is POST, then the entity body contains what the user entered into the form fields.
We would be remiss if we didn’t mention that a request generated with a form
does not necessarily have to use the POST method. Instead, HTML forms often use
the GET method and include the inputted data (in the form fields) in the requested
URL. For example, if a form uses the GET method, has two fields, and the inputs to
the two fields are monkeys and bananas, then the URL will have the structure
www.somesite.com/animalsearch?monkeys&bananas. In your day-today Web surfing, you have probably noticed extended URLs of this sort.
The HEAD method is similar to the GET method. When a server receives a
request with the HEAD method, it responds with an HTTP message but it leaves out
the requested object. Application developers often use the HEAD method for debugging. The PUT method is often used in conjunction with Web publishing tools. It
allows a user to upload an object to a specific path (directory) on a specific Web
server. The PUT method is also used by applications that need to upload objects
to Web servers. The DELETE method allows a user, or an application, to delete an
object on a Web server.

HTTP Response Message
Below we provide a typical HTTP response message. This response message could
be the response to the example request message just discussed.
HTTP/1.1 200 OK
Connection: close
Date: Tue, 18 Aug 2015 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 18 Aug 2015 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html!
(data data data data data ...)
Let’s take a careful look at this response message. It has three sections: an initial
status line, six header lines, and then the entity body. The entity body is the meat
of the message—it contains the requested object itself (represented by data data
data data data ...). The status line has three fields: the protocol version
field, a status code, and a corresponding status message. In this example, the status
line indicates that the server is using HTTP/1.1 and that everything is OK (that is, the
server has found, and is sending, the requested object).

103

104

CHAPTER 2

•

APPLICATION LAYER

Now let’s look at the header lines. The server uses the Connection: close
header line to tell the client that it is going to close the TCP connection after sending
the message. The Date: header line indicates the time and date when the HTTP
response was created and sent by the server. Note that this is not the time when
the object was created or last modified; it is the time when the server retrieves the
object from its file system, inserts the object into the response message, and sends the
response message. The Server: header line indicates that the message was generated by an Apache Web server; it is analogous to the User-agent: header line in
the HTTP request message. The Last-Modified: header line indicates the time
and date when the object was created or last modified. The Last-Modified:
header, which we will soon cover in more detail, is critical for object caching, both
in the local client and in network cache servers (also known as proxy servers). The
Content-Length: header line indicates the number of bytes in the object being
sent. The Content-Type: header line indicates that the object in the entity body
is HTML text. (The object type is officially indicated by the Content-Type:
header and not by the file extension.)
Having looked at an example, let’s now examine the general format of a response
message, which is shown in Figure 2.9. This general format of the response message
matches the previous example of a response message. Let’s say a few additional words
about status codes and their phrases. The status code and associated phrase indicate
the result of the request. Some common status codes and associated phrases include:
• 200 OK: Request succeeded and the information is returned in the response.
• 301 Moved Permanently: Requested object has been permanently moved;
the new URL is specified in Location: header of the response message. The
client software will automatically retrieve the new URL.

Status line

version

sp status code

sp

phrase

header field name: sp

value

cr lf

header field name: sp

value

cr lf

cr lf

Header lines

Blank line

cr lf

Entity body

Figure 2.9 ♦ General format of an HTTP response message

2.2

•

THE WEB AND HTTP

105

• 400 Bad Request: This is a generic error code indicating that the request
could not be understood by the server.
• 404 Not Found: The requested document does not exist on this server.
• 505 HTTP Version Not Supported: The requested HTTP protocol version is not supported by the server.
How would you like to see a real HTTP response message? This is highly recommended and very easy to do! First Telnet into your favorite Web server. Then
type in a one-line request message for some object that is housed on the server. For
example, if you have access to a command prompt, type:
telnet gaia.cs.umass.edu 80!
GET /kurose_ross/interactive/index.php HTTP/1.1
Host: gaia.cs.umass.edu
(Press the carriage return twice after typing the last line.) This opens a TCP connection to port 80 of the host gaia.cs.umass.edu and then sends the HTTP
request message. You should see a response message that includes the base HTML
file for the interactive homework problems for this textbook. If you’d rather just see
the HTTP message lines and not receive the object itself, replace GET with HEAD.
In this section, we discussed a number of header lines that can be used within
HTTP request and response messages. The HTTP specification defines many,
many more header lines that can be inserted by browsers, Web servers, and network cache servers. We have covered only a small number of the totality of header
lines. We’ll cover a few more below and another small number when we discuss
network Web caching in Section 2.2.5. A highly readable and comprehensive discussion of the HTTP protocol, including its headers and status codes, is given in
[Krishnamurthy 2001].
How does a browser decide which header lines to include in a request message?
How does a Web server decide which header lines to include in a response message? A browser will generate header lines as a function of the browser type and
version, the user configuration of the browser and whether the browser currently
has a cached, but possibly out-of-date, version of the object. Web servers behave
similarly: There are different products, versions, and configurations, all of which
influence which header lines are included in response messages.

2.2.4 User-Server Interaction: Cookies
We mentioned above that an HTTP server is stateless. This simplifies server design
and has permitted engineers to develop high-performance Web servers that can handle thousands of simultaneous TCP connections. However, it is often desirable for
a Web site to identify users, either because the server wishes to restrict user access

VideoNote
Using Wireshark to
investigate the HTTP
protocol

106

CHAPTER 2

•

APPLICATION LAYER

or because it wants to serve content as a function of the user identity. For these purposes, HTTP uses cookies. Cookies, defined in [RFC 6265], allow sites to keep track
of users. Most major commercial Web sites use cookies today.
As shown in Figure 2.10, cookie technology has four components: (1) a cookie
header line in the HTTP response message; (2) a cookie header line in the HTTP
request message; (3) a cookie file kept on the user’s end system and managed by
the user’s browser; and (4) a back-end database at the Web site. Using Figure 2.10,
let’s walk through an example of how cookies work. Suppose Susan, who always
Client host

Server host

usua

l ht

tp r

ebay: 8734

eque

st m

sg

nse
espo
tp r 78
t
h
l
usua okie: 16
co
Setusua
l
cook http req
ie:
1678 uest msg

amazon: 1678
ebay: 8734

u
One week later

entry in backend
database

Cookie-specific
action

sg
se m

n

espo

tp r

ht
sual

Server creates
ID 1678 for user

usua
l
cook http req
ie:
1678 uest msg

amazon: 1678
ebay: 8734

spon

l

usua

Time

re
http

se m

access

access

Cookie-specific
action

sg

Time

Key:
Cookie file

Figure 2.10 ♦ Keeping user state with cookies

2.2

•

THE WEB AND HTTP

accesses the Web using Internet Explorer from her home PC, contacts Amazon.com
for the first time. Let us suppose that in the past she has already visited the eBay site.
When the request comes into the Amazon Web server, the server creates a unique
identification number and creates an entry in its back-end database that is indexed
by the identification number. The Amazon Web server then responds to Susan’s
browser, including in the HTTP response a Set-cookie: header, which contains
the identification number. For example, the header line might be:
Set-cookie: 1678
When Susan’s browser receives the HTTP response message, it sees the
Set-cookie: header. The browser then appends a line to the special cookie file
that it manages. This line includes the hostname of the server and the identification
number in the Set-cookie: header. Note that the cookie file already has an entry
for eBay, since Susan has visited that site in the past. As Susan continues to browse
the Amazon site, each time she requests a Web page, her browser consults her cookie
file, extracts her identification number for this site, and puts a cookie header line that
includes the identification number in the HTTP request. Specifically, each of her
HTTP requests to the Amazon server includes the header line:
Cookie: 1678
In this manner, the Amazon server is able to track Susan’s activity at the Amazon
site. Although the Amazon Web site does not necessarily know Susan’s name, it
knows exactly which pages user 1678 visited, in which order, and at what times!
Amazon uses cookies to provide its shopping cart service—Amazon can maintain a
list of all of Susan’s intended purchases, so that she can pay for them collectively at
the end of the session.
If Susan returns to Amazon’s site, say, one week later, her browser will continue to put the header line Cookie: 1678 in the request messages. Amazon also
recommends products to Susan based on Web pages she has visited at Amazon in
the past. If Susan also registers herself with Amazon—providing full name, e-mail
address, postal address, and credit card information—Amazon can then include this
information in its database, thereby associating Susan’s name with her identification number (and all of the pages she has visited at the site in the past!). This is how
Amazon and other e-commerce sites provide “one-click shopping”—when Susan
chooses to purchase an item during a subsequent visit, she doesn’t need to re-enter
her name, credit card number, or address.
From this discussion, we see that cookies can be used to identify a user. The first
time a user visits a site, the user can provide a user identification (possibly his or her
name). During the subsequent sessions, the browser passes a cookie header to the
server, thereby identifying the user to the server. Cookies can thus be used to create
a user session layer on top of stateless HTTP. For example, when a user logs in to

107

108

CHAPTER 2

•

APPLICATION LAYER

a Web-based e-mail application (such as Hotmail), the browser sends cookie information to the server, permitting the server to identify the user throughout the user’s
session with the application.
Although cookies often simplify the Internet shopping experience for the user,
they are controversial because they can also be considered as an invasion of privacy.
As we just saw, using a combination of cookies and user-supplied account information, a Web site can learn a lot about a user and potentially sell this information to a
third party.

2.2.5 Web Caching
A Web cache—also called a proxy server—is a network entity that satisfies HTTP
requests on the behalf of an origin Web server. The Web cache has its own disk
storage and keeps copies of recently requested objects in this storage. As shown in
Figure 2.11, a user’s browser can be configured so that all of the user’s HTTP requests
are first directed to the Web cache [RFC 7234]. Once a browser is configured, each
browser request for an object is first directed to the Web cache. As an example,
suppose a browser is requesting the object http://www.someschool.edu/
campus.gif. Here is what happens:
1. The browser establishes a TCP connection to the Web cache and sends an HTTP
request for the object to the Web cache.
2. The Web cache checks to see if it has a copy of the object stored locally. If it
does, the Web cache returns the object within an HTTP response message to the
client browser.
3. If the Web cache does not have the object, the Web cache opens a TCP connection to the origin server, that is, to www.someschool.edu. The Web cache

HT
Client

HT

H

TP

TP

TT

res

T

ue

po

eq
Pr

HT
Client

req

ue

e
Pr

st

Proxy
server

HT

r
TP

nse

T
HT

st

HT

spo

nse

HT

ue

es
Pr

TP

TP

eq

req

res

st

po

ue

po

nse

Origin
server

st

nse
Origin
server

Figure 2.11 ♦ Clients requesting objects through a Web cache

2.2

•

THE WEB AND HTTP

then sends an HTTP request for the object into the cache-to-server TCP connection. After receiving this request, the origin server sends the object within an
HTTP response to the Web cache.
4. When the Web cache receives the object, it stores a copy in its local storage and
sends a copy, within an HTTP response message, to the client browser (over the
existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time. When it receives
requests from and sends responses to a browser, it is a server. When it sends requests
to and receives responses from an origin server, it is a client.
Typically a Web cache is purchased and installed by an ISP. For example, a university might install a cache on its campus network and configure all of the campus
browsers to point to the cache. Or a major residential ISP (such as Comcast) might
install one or more caches in its network and preconfigure its shipped browsers to
point to the installed caches.
Web caching has seen deployment in the Internet for two reasons. First, a Web
cache can substantially reduce the response time for a client request, particularly if
the bottleneck bandwidth between the client and the origin server is much less than
the bottleneck bandwidth between the client and the cache. If there is a high-speed
connection between the client and the cache, as there often is, and if the cache has
the requested object, then the cache will be able to deliver the object rapidly to the
client. Second, as we will soon illustrate with an example, Web caches can substantially reduce traffic on an institution’s access link to the Internet. By reducing
traffic, the institution (for example, a company or a university) does not have to
upgrade bandwidth as quickly, thereby reducing costs. Furthermore, Web caches
can substantially reduce Web traffic in the Internet as a whole, thereby improving
performance for all applications.
To gain a deeper understanding of the benefits of caches, let’s consider an example in the context of Figure 2.12. This figure shows two networks—the institutional
network and the rest of the public Internet. The institutional network is a high-speed
LAN. A router in the institutional network and a router in the Internet are connected
by a 15 Mbps link. The origin servers are attached to the Internet but are located all
over the globe. Suppose that the average object size is 1 Mbits and that the average
request rate from the institution’s browsers to the origin servers is 15 requests per
second. Suppose that the HTTP request messages are negligibly small and thus create no traffic in the networks or in the access link (from institutional router to Internet
router). Also suppose that the amount of time it takes from when the router on the
Internet side of the access link in Figure 2.12 forwards an HTTP request (within an
IP datagram) until it receives the response (typically within many IP datagrams) is
two seconds on average. Informally, we refer to this last delay as the “Internet delay.”
The total response time—that is, the time from the browser’s request of an
object until its receipt of the object—is the sum of the LAN delay, the access delay
(that is, the delay between the two routers), and the Internet delay. Let’s now do

109

110

CHAPTER 2

•

APPLICATION LAYER

Origin servers

Public Internet

15 Mbps access link

100 Mbps LAN

Institutional network

Figure 2.12 ♦ Bottleneck between an institutional network and the Internet
a very crude calculation to estimate this delay. The traffic intensity on the LAN
(see Section 1.4.2) is
(15 requests/sec) # (1 Mbits/request)/(100 Mbps) = 0.15
whereas the traffic intensity on the access link (from the Internet router to institution
router) is
(15 requests/sec) # (1 Mbits/request)/(15 Mbps) = 1
A traffic intensity of 0.15 on a LAN typically results in, at most, tens of milliseconds of delay; hence, we can neglect the LAN delay. However, as discussed in
Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link
in Figure 2.12), the delay on a link becomes very large and grows without bound.
Thus, the average response time to satisfy requests is going to be on the order of
minutes, if not more, which is unacceptable for the institution’s users. Clearly
something must be done.

2.2

•

THE WEB AND HTTP

One possible solution is to increase the access rate from 15 Mbps to, say, 100!Mbps.
This will lower the traffic intensity on the access link to 0.15, which translates to negligible delays between the two routers. In this case, the total response time will roughly
be two seconds, that is, the Internet delay. But this solution also means that the institution must upgrade its access link from 15 Mbps to 100 Mbps, a costly proposition.
Now consider the alternative solution of not upgrading the access link but
instead installing a Web cache in the institutional network. This solution is illustrated
in Figure 2.13. Hit rates—the fraction of requests that are satisfied by a cache—
typically range from 0.2 to 0.7 in practice. For illustrative purposes, let’s suppose
that the cache provides a hit rate of 0.4 for this institution. Because the clients and
the cache are connected to the same high-speed LAN, 40 percent of the requests will
be satisfied almost immediately, say, within 10 milliseconds, by the cache. Nevertheless, the remaining 60 percent of the requests still need to be satisfied by the origin
servers. But with only 60 percent of the requested objects passing through the access
link, the traffic intensity on the access link is reduced from 1.0 to 0.6. Typically, a
Origin servers

Public Internet

15 Mbps access link

100 Mbps LAN

Institutional network

Institutional
cache

Figure 2.13 ♦ Adding a cache to the institutional network

111

112

CHAPTER 2

•

APPLICATION LAYER

traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds,
on a 15 Mbps link. This delay is negligible compared with the two-second Internet
delay. Given these considerations, average delay therefore is
0.4 # (0.01 seconds) + 0.6 # (2.01 seconds)
which is just slightly greater than 1.2 seconds. Thus, this second solution provides an
even lower response time than the first solution, and it doesn’t require the institution
to upgrade its link to the Internet. The institution does, of course, have to purchase
and install a Web cache. But this cost is low—many caches use public-domain software that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are
increasingly playing an important role in the Internet. A CDN company installs many
geographically distributed caches throughout the Internet, thereby localizing much of
the traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs
(such as Google and Netflix). We will discuss CDNs in more detail in Section 2.6.

The Conditional GET
Although caching can reduce user-perceived response times, it introduces a new
problem—the copy of an object residing in the cache may be stale. In other words,
the object housed in the Web server may have been modified since the copy was
cached at the client. Fortunately, HTTP has a mechanism that allows a cache to
verify that its objects are up to date. This mechanism is called the conditional GET
[RFC 7232]. An HTTP request message is a so-called conditional GET message if
(1) the request message uses the GET method and (2) the request message includes an
If-Modified-Since: header line.
To illustrate how the conditional GET operates, let’s walk through an example.
First, on the behalf of a requesting browser, a proxy cache sends a request message
to a Web server:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
Second, the Web server sends a response message with the requested object to the
cache:
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif!
(data data data data data ...)

2.2

•

THE WEB AND HTTP

The cache forwards the object to the requesting browser but also caches the object
locally. Importantly, the cache also stores the last-modified date along with the
object. Third, one week later, another browser requests the same object via the cache,
and the object is still in the cache. Since this object may have been modified at the
Web server in the past week, the cache performs an up-to-date check by issuing a
conditional GET. Specifically, the cache sends:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
Note that the value of the If-modified-since: header line is exactly equal
to the value of the Last-Modified: header line that was sent by the server one
week ago. This conditional GET is telling the server to send the object only if the
object has been modified since the specified date. Suppose the object has not been
modified since 9 Sep 2015 09:23:24. Then, fourth, the Web server sends a response
message to the cache:
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)!
(empty entity body)
We see that in response to the conditional GET, the Web server still sends a
response message but does not include the requested object in the response message.
Including the requested object would only waste bandwidth and increase userperceived response time, particularly if the object is large. Note that this last response
message has 304 Not Modified in the status line, which tells the cache that it
can go ahead and forward its (the proxy cache’s) cached copy of the object to the
requesting browser.

2.2.6 HTTP/2
HTTP/2 [RFC 7540], standardized in 2015, was the first new version of HTTP since
HTTP/1.1, which was standardized in 1997. Since standardization, HTTP/2 has
taken off, with over 40% of the top 10 million websites supporting HTTP/2 in 2020
[W3Techs]. Most browsers—including Google Chrome, Internet Explorer, Safari,
Opera, and Firefox—also support HTTP/2.
The primary goals for HTTP/2 are to reduce perceived latency by enabling request
and response multiplexing over a single TCP connection, provide request prioritization
and server push, and provide efficient compression of HTTP header fields. HTTP/2
does not change HTTP methods, status codes, URLs, or header fields. Instead, HTTP/2
changes how the data is formatted and transported between the client and server.

113

114

CHAPTER 2

•

APPLICATION LAYER

To motivate the need for HTTP/2, recall that HTTP/1.1 uses persistent TCP
connections, allowing a Web page to be sent from server to client over a single TCP
connection. By having only one TCP connection per Web page, the number of sockets at the server is reduced and each transported Web page gets a fair share of the
network bandwidth (as discussed below). But developers of Web browsers quickly
discovered that sending all the objects in a Web page over a single TCP connection has a Head of Line (HOL) blocking problem. To understand HOL blocking,
consider a Web page that includes an HTML base page, a large video clip near the
top of Web page, and many small objects below the video. Further suppose there is
a low-to-medium speed bottleneck link (for example, a low-speed wireless link) on
the path between server and client. Using a single TCP connection, the video clip
will take a long time to pass through the bottleneck link, while the small objects are
delayed as they wait behind the video clip; that is, the video clip at the head of the
line blocks the small objects behind it. HTTP/1.1 browsers typically work around this
problem by opening multiple parallel TCP connections, thereby having objects in the
same web page sent in parallel to the browser. This way, the small objects can arrive
at and be rendered in the browser much faster, thereby reducing user-perceived delay.
TCP congestion control, discussed in detail in Chapter 3, also provides browsers an unintended incentive to use multiple parallel TCP connections rather than a
single persistent connection. Very roughly speaking, TCP congestion control aims to
give each TCP connection sharing a bottleneck link an equal share of the available
bandwidth of that link; so if there are n TCP connections operating over a bottleneck
link, then each connection approximately gets 1/nth of the bandwidth. By opening
multiple parallel TCP connections to transport a single Web page, the browser can
“cheat” and grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers
open up to six parallel TCP connections not only to circumvent HOL blocking but
also to obtain more bandwidth.
One of the primary goals of HTTP/2 is to get rid of (or at least reduce the number of) parallel TCP connections for transporting a single Web page. This not only
reduces the number of sockets that need to be open and maintained at servers, but
also allows TCP congestion control to operate as intended. But with only one TCP
connection to transport a Web page, HTTP/2 requires carefully designed mechanisms to avoid HOL blocking.

HTTP/2 Framing
The HTTP/2 solution for HOL blocking is to break each message into small frames, and
interleave the request and response messages on the same TCP connection. To understand this, consider again the example of a Web page consisting of one large video clip
and, say, 8 smaller objects. Thus the server will receive 9 concurrent requests from any
browser wanting to see this Web page. For each of these requests, the server needs to
send 9 competing HTTP response messages to the browser. Suppose all frames are of

2.2

•

THE WEB AND HTTP

fixed length, the video clip consists of 1000 frames, and each of the smaller objects
consists of two frames. With frame interleaving, after sending one frame from the
video clip, the first frames of each of the small objects are sent. Then after sending the
second frame of the video clip, the last frames of each of the small objects are sent.
Thus, all of the smaller objects are sent after sending a total of 18 frames. If interleaving were not used, the smaller objects would be sent only after sending 1016 frames.
Thus the HTTP/2 framing mechanism can significantly decrease user-perceived delay.
The ability to break down an HTTP message into independent frames, interleave them, and then reassemble them on the other end is the single most important
enhancement of HTTP/2. The framing is done by the framing sub-layer of the
HTTP/2 protocol. When a server wants to send an HTTP response, the response
is processed by the framing sub-layer, where it is broken down into frames. The
header field of the response becomes one frame, and the body of the message is
broken down into one for more additional frames. The frames of the response are
then interleaved by the framing sub-layer in the server with the frames of other
responses and sent over the single persistent TCP connection. As the frames arrive
at the client, they are first reassembled into the original response messages at the
framing sub-layer and then processed by the browser as usual. Similarly, a client’s
HTTP requests are broken into frames and interleaved.
In addition to breaking down each HTTP message into independent frames, the
framing sublayer also binary encodes the frames. Binary protocols are more efficient
to parse, lead to slightly smaller frames, and are less error-prone.

Response Message Prioritization and Server Pushing
Message prioritization allows developers to customize the relative priority of
requests to better optimize application performance. As we just learned, the framing sub-layer organizes messages into parallel streams of data destined to the same
requestor. When a client sends concurrent requests to a server, it can prioritize the
responses it is requesting by assigning a weight between 1 and 256 to each message.
The higher number indicates higher priority. Using these weights, the server can
send first the frames for the responses with the highest priority. In addition to this,
the client also states each message’s dependency on other messages by specifying
the ID of the message on which it depends.
Another feature of HTTP/2 is the ability for a server to send multiple responses
for a single client request. That is, in addition to the response to the original request,
the server can push additional objects to the client, without the client having to
request each one. This is possible since the HTML base page indicates the objects
that will be needed to fully render the Web page. So instead of waiting for the
HTTP requests for these objects, the server can analyze the HTML page, identify
the objects that are needed, and send them to the client before receiving explicit
requests for these objects. Server push eliminates the extra latency due to waiting
for the requests.

115

116

CHAPTER 2

•

APPLICATION LAYER

HTTP/3
QUIC, discussed in Chapter 3, is a new “transport” protocol that is implemented in
the application layer over the bare-bones UDP protocol. QUIC has several features
that are desirable for HTTP, such as message multiplexing (interleaving), per-stream
flow control, and low-latency connection establishment. HTTP/3 is yet a new HTTP
protocol that is designed to operate over QUIC. As of 2020, HTTP/3 is described
in Internet drafts and has not yet been fully standardized. Many of the HTTP/2 features (such as message interleaving) are subsumed by QUIC, allowing for a simpler,
streamlined design for HTTP/3.

2.3 Electronic Mail in the Internet
Electronic mail has been around since the beginning of the Internet. It was the most
popular application when the Internet was in its infancy [Segaller 1998], and has
become more elaborate and powerful over the years. It remains one of the Internet’s
most important and utilized applications.
As with ordinary postal mail, e-mail is an asynchronous communication
medium—people send and read messages when it is convenient for them, without
having to coordinate with other people’s schedules. In contrast with postal mail,
electronic mail is fast, easy to distribute, and inexpensive. Modern e-mail has
many powerful features, including messages with attachments, hyperlinks, HTMLformatted text, and embedded photos.
In this section, we examine the application-layer protocols that are at the heart
of Internet e-mail. But before we jump into an in-depth discussion of these protocols,
let’s take a high-level view of the Internet mail system and its key components.
Figure 2.14 presents a high-level view of the Internet mail system. We see from
this diagram that it has three major components: user agents, mail servers, and the
Simple Mail Transfer Protocol (SMTP). We now describe each of these components in the context of a sender, Alice, sending an e-mail message to a recipient,
Bob. User agents allow users to read, reply to, forward, save, and compose messages.
Examples of user agents for e-mail include Microsoft Outlook, Apple Mail, Webbased Gmail, the Gmail App running in a smartphone, and so on. When Alice is
finished composing her message, her user agent sends the message to her mail server,
where the message is placed in the mail server’s outgoing message queue. When Bob
wants to read a message, his user agent retrieves the message from his mailbox in his
mail server.
Mail servers form the core of the e-mail infrastructure. Each recipient, such
as Bob, has a mailbox located in one of the mail servers. Bob’s mailbox manages
and maintains the messages that have been sent to him. A typical message starts its
journey in the sender’s user agent, then travels to the sender’s mail server, and then

2.3

•

ELECTRONIC MAIL IN THE INTERNET

Mail server
User agent

SM

SMTP

TP

SM

TP

Mail server
User agent

Mail server
User agent

User agent
User agent

User agent

Key:
Outgoing
message queue

User mailbox

Figure 2.14 ♦ A high-level view of the Internet e-mail system
travels to the recipient’s mail server, where it is deposited in the recipient’s mailbox.
When Bob wants to access the messages in his mailbox, the mail server containing
his mailbox authenticates Bob (with his username and password). Alice’s mail server
must also deal with failures in Bob’s mail server. If Alice’s server cannot deliver
mail to Bob’s server, Alice’s server holds the message in a message queue and
attempts to transfer the message later. Reattempts are often done every 30 minutes
or so; if there is no success after several days, the server removes the message and
notifies the sender (Alice) with an e-mail message.
SMTP is the principal application-layer protocol for Internet electronic mail. It
uses the reliable data transfer service of TCP to transfer mail from the sender’s mail
server to the recipient’s mail server. As with most application-layer protocols, SMTP
has two sides: a client side, which executes on the sender’s mail server, and a server
side, which executes on the recipient’s mail server. Both the client and server sides of

117

118

CHAPTER 2

•

APPLICATION LAYER

SMTP run on every mail server. When a mail server sends mail to other mail servers,
it acts as an SMTP client. When a mail server receives mail from other mail servers,
it acts as an SMTP server.

2.3.1 SMTP
SMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As mentioned above, SMTP transfers messages from senders’ mail servers to the recipients’
mail servers. SMTP is much older than HTTP. (The original SMTP RFC dates back
to 1982, and SMTP was around long before that.) Although SMTP has numerous
wonderful qualities, as evidenced by its ubiquity in the Internet, it is nevertheless
a legacy technology that possesses certain archaic characteristics. For example, it
restricts the body (not just the headers) of all mail messages to simple 7-bit ASCII.
This restriction made sense in the early 1980s when transmission capacity was scarce
and no one was e-mailing large attachments or large image, audio, or video files. But
today, in the multimedia era, the 7-bit ASCII restriction is a bit of a pain—it requires
binary multimedia data to be encoded to ASCII before being sent over SMTP; and it
requires the corresponding ASCII message to be decoded back to binary after SMTP
transport. Recall from Section 2.2 that HTTP does not require multimedia data to be
ASCII encoded before transfer.
To illustrate the basic operation of SMTP, let’s walk through a common scenario. Suppose Alice wants to send Bob a simple ASCII message.
1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address (for
example, bob@someschool.edu), composes a message, and instructs the
user agent to send the message.
2. Alice’s user agent sends the message to her mail server, where it is placed in a
message queue.
3. The client side of SMTP, running on Alice’s mail server, sees the message in the
message queue. It opens a TCP connection to an SMTP server, running on Bob’s
mail server.
4. After some initial SMTP handshaking, the SMTP client sends Alice’s message
into the TCP connection.
5. At Bob’s mail server, the server side of SMTP receives the message. Bob’s mail
server then places the message in Bob’s mailbox.
6. Bob invokes his user agent to read the message at his convenience.
The scenario is summarized in Figure 2.15.
It is important to observe that SMTP does not normally use intermediate mail servers for sending mail, even when the two mail servers are located at opposite ends of
the world. If Alice’s server is in Hong Kong and Bob’s server is in St. Louis, the TCP
connection is a direct connection between the Hong Kong and St. Louis servers. In

2.3

Alice’s
mail server

1
Alice’s
agent

•

2

3

ELECTRONIC MAIL IN THE INTERNET

Bob’s
mail server
4
SMTP

6
5

Key:
Message queue

User mailbox

Figure 2.15 ♦ Alice sends a message to Bob
particular, if Bob’s mail server is down, the message remains in Alice’s mail server and
waits for a new attempt—the message does not get placed in some intermediate mail
server.
Let’s now take a closer look at how SMTP transfers a message from a sending mail server to a receiving mail server. We will see that the SMTP protocol has many similarities with protocols that are used for face-to-face human
interaction. First, the client SMTP (running on the sending mail server host) has
TCP establish a connection to port 25 at the server SMTP (running on the receiving mail server host). If the server is down, the client tries again later. Once
this connection is established, the server and client perform some applicationlayer handshaking—just as humans often introduce themselves before transferring information from one to another, SMTP clients and servers introduce
themselves before transferring information. During this SMTP handshaking phase,
the SMTP client indicates the e-mail address of the sender (the person who generated the message) and the e-mail address of the recipient. Once the SMTP client and
server have introduced themselves to each other, the client sends the message. SMTP
can count on the reliable data transfer service of TCP to get the message to the server
without errors. The client then repeats this process over the same TCP connection if
it has other messages to send to the server; otherwise, it instructs TCP to close the
connection.
Let’s next take a look at an example transcript of messages exchanged between an
SMTP client (C) and an SMTP server (S). The hostname of the client is crepes.fr
and the hostname of the server is hamburger.edu. The ASCII text lines prefaced
with C: are exactly the lines the client sends into its TCP socket, and the ASCII text
lines prefaced with S: are exactly the lines the server sends into its TCP socket. The
following transcript begins as soon as the TCP connection is established.
S:! ! 220 hamburger.edu
C:! ! HELO crepes.fr

Bob’s
agent

119

120

CHAPTER 2

•

APPLICATION LAYER

S:! ! 250 Hello crepes.fr, pleased to meet you
C:! ! MAIL FROM: <alice@crepes.fr>
S:! ! 250 alice@crepes.fr ... Sender ok
C:! ! RCPT TO: <bob@hamburger.edu>
S:! ! 250 bob@hamburger.edu ... Recipient ok
C:! ! DATA
S:! ! 354 Enter mail, end with ”.” on a line by itself
C:! ! Do you like ketchup?
C:! ! How about pickles?
C:! ! .
S:! ! 250 Message accepted for delivery
C:! ! QUIT
S:! ! 221 hamburger.edu closing connection
In the example above, the client sends a message (“Do you like ketchup?
How about pickles?”) from mail server crepes.fr to mail server
hamburger.edu. As part of the dialogue, the client issued five commands:
HELO (an abbreviation for HELLO), MAIL FROM, RCPT TO, DATA, and QUIT.
These commands are self-explanatory. The client also sends a line consisting of a
single period, which indicates the end of the message to the server. (In ASCII jargon, each message ends with CRLF.CRLF, where CR and LF stand for carriage
return and line feed, respectively.) The server issues replies to each command,
with each reply having a reply code and some (optional) English-language explanation. We mention here that SMTP uses persistent connections: If the sending
mail server has several messages to send to the same receiving mail server, it can
send all of the messages over the same TCP connection. For each message, the
client begins the process with a new MAIL FROM: crepes.fr, designates the
end of message with an isolated period, and issues QUIT only after all messages
have been sent.
It is highly recommended that you use Telnet to carry out a direct dialogue with
an SMTP server. To do this, issue
telnet serverName 25
where serverName is the name of a local mail server. When you do this, you are
simply establishing a TCP connection between your local host and the mail server.
After typing this line, you should immediately receive the 220 reply from the
server. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA,
CRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended
that you do Programming Assignment 3 at the end of this chapter. In that assignment, you’ll build a simple user agent that implements the client side of SMTP. It
will allow you to send an e-mail message to an arbitrary recipient via a local mail
server.

2.3

•

ELECTRONIC MAIL IN THE INTERNET

2.3.2 Mail Message Formats
When Alice writes an ordinary snail-mail letter to Bob, she may include all kinds
of peripheral header information at the top of the letter, such as Bob’s address, her
own return address, and the date. Similarly, when an e-mail message is sent from
one person to another, a header containing peripheral information precedes the
body of the message itself. This peripheral information is contained in a series of
header lines, which are defined in RFC 5322. The header lines and the body of the
message are separated by a blank line (that is, by CRLF). RFC 5322 specifies the
exact format for mail header lines as well as their semantic interpretations. As with
HTTP, each header line contains readable text, consisting of a keyword followed
by a colon followed by a value. Some of the keywords are required and others are
optional. Every header must have a From: header line and a To: header line;
a header may include a Subject: header line as well as other optional header
lines. It is important to note that these header lines are different from the SMTP
commands we studied in Section 2.3.1 (even though they contain some common
words such as “from” and “to”). The commands in that section were part of the
SMTP handshaking protocol; the header lines examined in this section are part of
the mail message itself.
A typical message header looks like this:
From: alice@crepes.fr
To: bob@hamburger.edu
Subject: Searching for the meaning of life.
After the message header, a blank line follows; then the message body (in ASCII)
follows. You should use Telnet to send a message to a mail server that contains
some header lines, including the Subject: header line. To do this, issue telnet
serverName 25, as discussed in Section 2.3.1.

2.3.3 Mail Access Protocols
Once SMTP delivers the message from Alice’s mail server to Bob’s mail server, the
message is placed in Bob’s mailbox. Given that Bob (the recipient) executes his user
agent on his local host (e.g., smartphone or PC), it is natural to consider placing a mail
server on his local host as well. With this approach, Alice’s mail server would dialogue directly with Bob’s PC. There is a problem with this approach, however. Recall
that a mail server manages mailboxes and runs the client and server sides of SMTP.
If Bob’s mail server were to reside on his local host, then Bob’s host would have to
remain always on, and connected to the Internet, in order to receive new mail, which
can arrive at any time. This is impractical for many Internet users. Instead, a typical
user runs a user agent on the local host but accesses its mailbox stored on an alwayson shared mail server. This mail server is shared with other users.

121

122

CHAPTER 2

•

APPLICATION LAYER

Alice’s
mail server
Alice’s
agent

SMTP
or
HTTP

Bob’s
mail server
SMTP

HTTP
or
IMAP

Bob’s
agent

Figure 2.16 ♦ E-mail protocols and their communicating entities
Now let’s consider the path an e-mail message takes when it is sent from Alice
to Bob. We just learned that at some point along the path the e-mail message needs to
be deposited in Bob’s mail server. This could be done simply by having Alice’s user
agent send the message directly to Bob’s mail server. However, typically the sender’s user agent does not dialogue directly with the recipient’s mail server. Instead, as
shown in Figure 2.16, Alice’s user agent uses SMTP or HTTP to deliver the e-mail
message into her mail server, then Alice’s mail server uses SMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server. Why the two-step procedure?
Primarily because without relaying through Alice’s mail server, Alice’s user agent
doesn’t have any recourse to an unreachable destination mail server. By having Alice
first deposit the e-mail in her own mail server, Alice’s mail server can repeatedly try
to send the message to Bob’s mail server, say every 30 minutes, until Bob’s mail
server becomes operational. (And if Alice’s mail server is down, then she has the
recourse of complaining to her system administrator!)
But there is still one missing piece to the puzzle! How does a recipient like Bob,
running a user agent on his local host , obtain his messages, which are sitting in a mail
server? Note that Bob’s user agent can’t use SMTP to obtain the messages because
obtaining the messages is a pull operation, whereas SMTP is a push protocol.
Today, there are two common ways for Bob to retrieve his e-mail from a mail
server. If Bob is using Web-based e-mail or a smartphone app (such as Gmail), then
the user agent will use HTTP to retrieve Bob’s e-mail. This case requires Bob’s mail
server to have an HTTP interface as well as an SMTP interface (to communicate with
Alice’s mail server). The alternative method, typically used with mail clients such
as Microsoft Outlook, is to use the Internet Mail Access Protocol (IMAP) defined
in RFC 3501. Both the HTTP and IMAP approaches allow Bob to manage folders,
maintained in Bob’s mail server. Bob can move messages into the folders he creates,
delete messages, mark messages as important, and so on.

2.4 DNS—The Internet’s Directory Service
We human beings can be identified in many ways. For example, we can be identified by the names that appear on our birth certificates. We can be identified by
our social security numbers. We can be identified by our driver’s license numbers.

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

Although each can be used to identify people, within a given context one identifier
may be more appropriate than another. For example, the computers at the IRS (the
infamous tax-collecting agency in the United States) prefer to use fixed-length social
security numbers rather than birth certificate names. On the other hand, ordinary
people prefer the more mnemonic birth certificate names rather than social security
numbers. (Indeed, can you imagine saying, “Hi. My name is 132-67-9875. Please
meet my husband, 178-87-1146.”)
Just as humans can be identified in many ways, so too can Internet hosts. One
identifier for a host is its hostname. Hostnames—such as www.facebook.com,
www.google.com, gaia.cs.umass.edu—are mnemonic and are therefore
appreciated by humans. However, hostnames provide little, if any, information about
the location within the Internet of the host. (A hostname such as www.eurecom.
fr, which ends with the country code .fr, tells us that the host is probably in
France, but doesn’t say much more.) Furthermore, because hostnames can consist of
variable-length alphanumeric characters, they would be difficult to process by routers. For these reasons, hosts are also identified by so-called IP addresses.
We discuss IP addresses in some detail in Chapter 4, but it is useful to say a
few brief words about them now. An IP address consists of four bytes and has a
rigid hierarchical structure. An IP address looks like 121.7.106.83, where each
period separates one of the bytes expressed in decimal notation from 0 to 255. An IP
address is hierarchical because as we scan the address from left to right, we obtain
more and more specific information about where the host is located in the Internet
(that is, within which network, in the network of networks). Similarly, when we scan
a postal address from bottom to top, we obtain more and more specific information
about where the addressee is located.

2.4.1 Services Provided by DNS
We have just seen that there are two ways to identify a host—by a hostname and
by an IP address. People prefer the more mnemonic hostname identifier, while
routers prefer fixed-length, hierarchically structured IP addresses. In order to reconcile these preferences, we need a directory service that translates hostnames to
IP addresses. This is the main task of the Internet’s domain name system (DNS).
The DNS is (1) a distributed database implemented in a hierarchy of DNS servers,
and (2) an application-layer protocol that allows hosts to query the distributed
database. The DNS servers are often UNIX machines running the Berkeley Internet Name Domain (BIND) software [BIND 2020]. The DNS protocol runs over
UDP and uses port 53.
DNS is commonly employed by other application-layer protocols, including
HTTP and SMTP, to translate user-supplied hostnames to IP addresses. As an example, consider what happens when a browser (that is, an HTTP client), running on
some user’s host, requests the URL www.someschool.edu/index.html. In
order for the user’s host to be able to send an HTTP request message to the Web

123

124

CHAPTER 2

•

APPLICATION LAYER

server www.someschool.edu, the user’s host must first obtain the IP address of
www.someschool.edu. This is done as follows.
1. The same user machine runs the client side of the DNS application.
2. The browser extracts the hostname, www.someschool.edu, from the URL
and passes the hostname to the client side of the DNS application.
3. The DNS client sends a query containing the hostname to a DNS server.
4. The DNS client eventually receives a reply, which includes the IP address for
the hostname.
5. Once the browser receives the IP address from DNS, it can initiate a TCP connection to the HTTP server process located at port 80 at that IP address.
We see from this example that DNS adds an additional delay—sometimes
substantial—to the Internet applications that use it. Fortunately, as we discuss below,
the desired IP address is often cached in a “nearby” DNS server, which helps to
reduce DNS network traffic as well as the average DNS delay.
DNS provides a few other important services in addition to translating hostnames to IP addresses:
• Host aliasing. A host with a complicated hostname can have one or more
alias names. For example, a hostname such as relay1.west-coast
.enterprise.com could have, say, two aliases such as enterprise.com
and www.enterprise.com. In this case, the hostname relay1
.west-coast.enterprise.com is said to be a canonical hostname. Alias
hostnames, when present, are typically more mnemonic than canonical hostnames. DNS can be invoked by an application to obtain the canonical hostname
for a supplied alias hostname as well as the IP address of the host.
• Mail server aliasing. For obvious reasons, it is highly desirable that e-mail
addresses be mnemonic. For example, if Bob has an account with Yahoo Mail,
Bob’s e-mail address might be as simple as bob@yahoo.com. However, the
hostname of the Yahoo mail server is more complicated and much less mnemonic
than simply yahoo.com (for example, the canonical hostname might be something like relay1.west-coast.yahoo.com). DNS can be invoked by a
mail application to obtain the canonical hostname for a supplied alias hostname
as well as the IP address of the host. In fact, the MX record (see below) permits a
company’s mail server and Web server to have identical (aliased) hostnames; for
example, a company’s Web server and mail server can both be called enterprise.com.
• Load distribution. DNS is also used to perform load distribution among replicated servers, such as replicated Web servers. Busy sites, such as cnn.com, are
replicated over multiple servers, with each server running on a different end system and each having a different IP address. For replicated Web servers, a set of IP

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

PRINCIPLES IN PRACTICE
DN S: CRIT ICA L NET WO RK F U NCT IO NS VI A TH E C L I ENT-S ER VER P A R A D I G M
Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it
(1) runs between communicating end systems using the client-server paradigm and
(2) relies on an underlying end-to-end transport protocol to transfer DNS messages between
communicating end systems. In another sense, however, the role of the DNS is quite different from Web, file transfer, and e-mail applications. Unlike these applications, the DNS is
not an application with which a user directly interacts. Instead, the DNS provides a core
Internet function—namely, translating hostnames to their underlying IP addresses, for user
applications and other software in the Internet. We noted in Section 1.2 that much of the
complexity in the Internet architecture is located at the “edges” of the network. The DNS,
which implements the critical name-to-address translation process using clients and servers
located at the edge of the network, is yet another example of that design philosophy.

addresses is thus associated with one alias hostname. The DNS database contains
this set of IP addresses. When clients make a DNS query for a name mapped to a
set of addresses, the server responds with the entire set of IP addresses, but rotates
the ordering of the addresses within each reply. Because a client typically sends
its HTTP request message to the IP address that is listed first in the set, DNS rotation distributes the traffic among the replicated servers. DNS rotation is also used
for e-mail so that multiple mail servers can have the same alias name. Also, content distribution companies such as Akamai have used DNS in more sophisticated
ways [Dilley 2002] to provide Web content distribution (see Section 2.6.3).
The DNS is specified in RFC 1034 and RFC 1035, and updated in several additional RFCs. It is a complex system, and we only touch upon key aspects of its
operation here. The interested reader is referred to these RFCs and the book by Albitz
and Liu [Albitz 1993]; see also the retrospective paper [Mockapetris 1988], which
provides a nice description of the what and why of DNS, and [Mockapetris 2005].

2.4.2 Overview of How DNS Works
We now present a high-level overview of how DNS works. Our discussion will focus
on the hostname-to-IP-address translation service.
Suppose that some application (such as a Web browser or a mail client) running
in a user’s host needs to translate a hostname to an IP address. The application will
invoke the client side of DNS, specifying the hostname that needs to be translated.
(On many UNIX-based machines, gethostbyname() is the function call that
an application calls in order to perform the translation.) DNS in the user’s host then

125

126

CHAPTER 2

•

APPLICATION LAYER

takes over, sending a query message into the network. All DNS query and reply messages are sent within UDP datagrams to port 53. After a delay, ranging from milliseconds to seconds, DNS in the user’s host receives a DNS reply message that provides
the desired mapping. This mapping is then passed to the invoking application. Thus,
from the perspective of the invoking application in the user’s host, DNS is a black
box providing a simple, straightforward translation service. But in fact, the black box
that implements the service is complex, consisting of a large number of DNS servers
distributed around the globe, as well as an application-layer protocol that specifies
how the DNS servers and querying hosts communicate.
A simple design for DNS would have one DNS server that contains all the mappings. In this centralized design, clients simply direct all queries to the single DNS
server, and the DNS server responds directly to the querying clients. Although the
simplicity of this design is attractive, it is inappropriate for today’s Internet, with its
vast (and growing) number of hosts. The problems with a centralized design include:
• A single point of failure. If the DNS server crashes, so does the entire Internet!
• Traffic volume. A single DNS server would have to handle all DNS queries (for
all the HTTP requests and e-mail messages generated from hundreds of millions
of hosts).
• Distant centralized database. A single DNS server cannot be “close to” all the
querying clients. If we put the single DNS server in New York City, then all queries from Australia must travel to the other side of the globe, perhaps over slow
and congested links. This can lead to significant delays.
• Maintenance. The single DNS server would have to keep records for all Internet
hosts. Not only would this centralized database be huge, but it would have to be
updated frequently to account for every new host.
In summary, a centralized database in a single DNS server simply doesn’t scale.
Consequently, the DNS is distributed by design. In fact, the DNS is a wonderful
example of how a distributed database can be implemented in the Internet.

A Distributed, Hierarchical Database
In order to deal with the issue of scale, the DNS uses a large number of servers,
organized in a hierarchical fashion and distributed around the world. No single DNS
server has all of the mappings for all of the hosts in the Internet. Instead, the mappings are distributed across the DNS servers. To a first approximation, there are three
classes of DNS servers—root DNS servers, top-level domain (TLD) DNS servers,
and authoritative DNS servers—organized in a hierarchy as shown in Figure 2.17.
To understand how these three classes of servers interact, suppose a DNS client
wants to determine the IP address for the hostname www.amazon.com. To a first
approximation, the following events will take place. The client first contacts one of

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

Root DNS servers

com DNS servers

facebook.com
DNS servers

amazon.com
DNS servers

org DNS servers

pbs.org
DNS servers

edu DNS servers

nyu.edu
DNS servers

umass.edu
DNS servers

Figure 2.17 ♦ Portion of the hierarchy of DNS servers
the root servers, which returns IP addresses for TLD servers for the top-level domain
com. The client then contacts one of these TLD servers, which returns the IP address
of an authoritative server for amazon.com. Finally, the client contacts one of the
authoritative servers for amazon.com, which returns the IP address for the hostname www.amazon.com. We’ll soon examine this DNS lookup process in more
detail. But let’s first take a closer look at these three classes of DNS servers:
• Root DNS servers. There are more than 1000 root servers instances scattered all
over the world, as shown in Figure 2.18. These root servers are copies of 13!different root servers, managed by 12 different organizations, and coordinated
through the Internet Assigned Numbers Authority [IANA 2020]. The full list
of root name servers, along with the organizations that manage them and their
IP!addresses can be found at [Root Servers 2020]. Root name servers provide
the!IP addresses of the TLD servers.
• Top-level domain (TLD) servers. For each of the top-level domains—top-level
domains such as com, org, net, edu, and gov, and all of the country top-level
domains such as uk, fr, ca, and jp—there is TLD server (or server cluster). The
company Verisign Global Registry Services maintains the TLD servers for the
com top-level domain, and the company Educause maintains the TLD servers for
the edu top-level domain. The network infrastructure supporting a TLD can be
large and complex; see [Osterweil 2012] for a nice overview of the Verisign network. See [TLD list 2020] for a list of all top-level domains. TLD servers provide
the IP addresses for authoritative DNS servers.
• Authoritative DNS servers. Every organization with publicly accessible hosts
(such as Web servers and mail servers) on the Internet must provide publicly
accessible DNS records that map the names of those hosts to IP addresses. An
organization’s authoritative DNS server houses these DNS records. An organization can choose to implement its own authoritative DNS server to hold these
records; alternatively, the organization can pay to have these records stored in an

127

128

CHAPTER 2

•

APPLICATION LAYER

Key:
0 Servers
1–10 Servers
11–20 Servers
21+ Servers

Figure 2.18 ♦ DNS root servers in 2020
authoritative DNS server of some service provider. Most universities and large
companies implement and maintain their own primary and secondary (backup)
authoritative DNS server.
The root, TLD, and authoritative DNS servers all belong to the hierarchy of
DNS servers, as shown in Figure 2.17. There is another important type of DNS
server called the local DNS server. A local DNS server does not strictly belong to
the hierarchy of servers but is nevertheless central to the DNS architecture. Each
ISP—such as a residential ISP or an institutional ISP—has a local DNS server (also
called a default name server). When a host connects to an ISP, the ISP provides
the host with the IP addresses of one or more of its local DNS servers (typically
through DHCP, which is discussed in Chapter 4). You can easily determine the IP
address of your local DNS server by accessing network status windows in Windows or UNIX. A host’s local DNS server is typically “close to” the host. For an
institutional ISP, the local DNS server may be on the same LAN as the host; for a
residential ISP, it is typically separated from the host by no more than a few routers. When a host makes a DNS query, the query is sent to the local DNS server,
which acts a proxy, forwarding the query into the DNS server hierarchy, as we’ll
discuss in more detail below.
Let’s take a look at a simple example. Suppose the host cse.nyu.edu desires
the IP address of gaia.cs.umass.edu. Also suppose that NYU’s local DNS
server for cse.nyu.edu is called dns.nyu.edu and that an authoritative DNS
server for gaia.cs.umass.edu is called dns.umass.edu. As shown in

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

Root DNS server

2
3
4
5
Local DNS server
dns.nyu.edu

TLD DNS server

7

1

6

8

Authoritative DNS server
dns.umass.edu
Requesting host
cse.nyu.edu
gaia.cs.umass.edu

Figure 2.19 ♦ Interaction of the various DNS servers
Figure 2.19, the host cse.nyu.edu first sends a DNS query message to its local
DNS server, dns.nyu.edu. The query message contains the hostname to be translated, namely, gaia.cs.umass.edu. The local DNS server forwards the query
message to a root DNS server. The root DNS server takes note of the edu suffix and
returns to the local DNS server a list of IP addresses for TLD servers responsible
for edu. The local DNS server then resends the query message to one of these TLD
servers. The TLD server takes note of the umass.edu suffix and responds with
the IP address of the authoritative DNS server for the University of Massachusetts,
namely, dns.umass.edu. Finally, the local DNS server resends the query message directly to dns.umass.edu, which responds with the IP address of gaia
.cs.umass.edu. Note that in this example, in order to obtain the mapping for one
hostname, eight DNS messages were sent: four query messages and four reply messages! We’ll soon see how DNS caching reduces this query traffic.
Our previous example assumed that the TLD server knows the authoritative DNS
server for the hostname. In general, this is not always true. Instead, the TLD server

129

130

CHAPTER 2

•

APPLICATION LAYER

may know only of an intermediate DNS server, which in turn knows the authoritative DNS server for the hostname. For example, suppose again that the University of
Massachusetts has a DNS server for the university, called dns.umass.edu. Also
suppose that each of the departments at the University of Massachusetts has its own
DNS server, and that each departmental DNS server is authoritative for all hosts in
the department. In this case, when the intermediate DNS server, dns.umass.edu,
receives a query for a host with a hostname ending with cs.umass.edu, it returns
to dns.nyu.edu the IP address of dns.cs.umass.edu, which is authoritative
for all hostnames ending with cs.umass.edu. The local DNS server dns.nyu
.edu then sends the query to the authoritative DNS server, which returns the desired
mapping to the local DNS server, which in turn returns the mapping to the requesting
host. In this case, a total of 10 DNS messages are sent!
The example shown in Figure 2.19 makes use of both recursive queries and
iterative queries. The query sent from cse.nyu.edu to dns.nyu.edu is a
recursive query, since the query asks dns.nyu.edu to obtain the mapping on its
behalf. However, the subsequent three queries are iterative since all of the replies
are directly returned to dns.nyu.edu. In theory, any DNS query can be iterative or recursive. For example, Figure 2.20 shows a DNS query chain for which all
of the queries are recursive. In practice, the queries typically follow the pattern in
Figure 2.19: The query from the requesting host to the local DNS server is recursive,
and the remaining queries are iterative.

DNS Caching
Our discussion thus far has ignored DNS caching, a critically important feature
of the DNS system. In truth, DNS extensively exploits DNS caching in order to
improve the delay performance and to reduce the number of DNS messages
ricocheting around the Internet. The idea behind DNS caching is very simple. In a
query chain, when a DNS server receives a DNS reply (containing, for example, a
mapping from a hostname to an IP address), it can cache the mapping in its local
memory. For example, in Figure 2.19, each time the local DNS server dns.nyu.edu
receives a reply from some DNS server, it can cache any of the information contained
in the reply. If a hostname/IP address pair is cached in a DNS server and another
query arrives to the DNS server for the same hostname, the DNS server can provide
the desired IP address, even if it is not authoritative for the hostname. Because hosts
and mappings between hostnames and IP addresses are by no means permanent,
DNS servers discard cached information after a period of time (often set to two days).
As an example, suppose that a host apricot.nyu.edu queries dns.nyu.edu
for the IP address for the hostname cnn.com. Furthermore, suppose that a few hours
later, another NYU host, say, kiwi.nyu.edu, also queries dns.nyu.edu
with the same hostname. Because of caching, the local DNS server will be able
to immediately return the IP address of cnn.com to this second requesting
host! without having to query any other DNS servers. A local DNS server can

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

Root DNS server

2
7

3

6

Local DNS server
dns.nyu.edu

TLD DNS server

5
1

4
8

Requesting host
cse.nyu.edu

Authoritative DNS server
dns.umass.edu

gaia.cs.umass.edu

Figure 2.20 ♦ Recursive queries in DNS
also cache the IP addresses of TLD servers, thereby allowing the local DNS server
to bypass the root DNS servers in a query chain. In fact, because of caching, root
servers are bypassed for all but a very small fraction of DNS queries.

2.4.3 DNS Records and Messages
The DNS servers that together implement the DNS distributed database store
resource records (RRs), including RRs that provide hostname-to-IP address mappings. Each DNS reply message carries one or more resource records. In this and
the following subsection, we provide a brief overview of DNS resource records and
messages; more details can be found in [Albitz 1993] or in the DNS RFCs [RFC
1034; RFC 1035].

131

132

CHAPTER 2

•

APPLICATION LAYER

A resource record is a four-tuple that contains the following fields:
(Name, Value, Type, TTL)
TTL is the time to live of the resource record; it determines when a resource should
be removed from a cache. In the example records given below, we ignore the TTL
field. The meaning of Name and Value depend on Type:
• If Type=A, then Name is a hostname and Value is the IP address for the hostname. Thus, a Type A record provides the standard hostname-to-IP address mapping. As an example, (relay1.bar.foo.com, 145.37.93.126, A) is
a Type A record.
• If Type=NS, then Name is a domain (such as foo.com) and Value is the hostname of an authoritative DNS server that knows how to obtain the IP addresses
for hosts in the domain. This record is used to route DNS queries further along in
the query chain. As an example, (foo.com, dns.foo.com, NS) is a Type
NS record.
• If Type=CNAME, then Value is a canonical hostname for the alias hostname
Name. This record can provide querying hosts the canonical name for a hostname. As an example, (foo.com, relay1.bar.foo.com, CNAME) is a
CNAME record.
• If Type=MX, then Value is the canonical name of a mail server that has an alias
hostname Name. As an example, (foo.com, mail.bar.foo.com, MX)
is an MX record. MX records allow the hostnames of mail servers to have simple
aliases. Note that by using the MX record, a company can have the same aliased
name for its mail server and for one of its other servers (such as its Web server).
To obtain the canonical name for the mail server, a DNS client would query for
an MX record; to obtain the canonical name for the other server, the DNS client
would query for the CNAME record.
If a DNS server is authoritative for a particular hostname, then the DNS server
will contain a Type A record for the hostname. (Even if the DNS server is not authoritative, it may contain a Type A record in its cache.) If a server is not authoritative
for a hostname, then the server will contain a Type NS record for the domain that
includes the hostname; it will also contain a Type A record that provides the IP address
of the DNS server in the Value field of the NS record. As an example, suppose an
edu TLD server is not authoritative for the host gaia.cs.umass.edu. Then this
server will contain a record for a domain that includes the host gaia.cs.umass
.edu, for example, (umass.edu, dns.umass.edu, NS). The edu
TLD server would also contain a Type A record, which maps the DNS server
dns.umass.edu to an IP address, for example, (dns.umass.edu,
128.119.40.111, A).

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

DNS Messages
Earlier in this section, we referred to DNS query and reply messages. These are the
only two kinds of DNS messages. Furthermore, both query and reply messages have
the same format, as shown in Figure 2.21.The semantics of the various fields in a
DNS message are as follows:
• The first 12 bytes is the header section, which has a number of fields. The first
field is a 16-bit number that identifies the query. This identifier is copied into the
reply message to a query, allowing the client to match received replies with sent
queries. There are a number of flags in the flag field. A 1-bit query/reply flag indicates whether the message is a query (0) or a reply (1). A 1-bit authoritative flag is
set in a reply message when a DNS server is an authoritative server for a queried
name. A 1-bit recursion-desired flag is set when a client (host or DNS server)
desires that the DNS server perform recursion when it doesn’t have the record. A
1-bit recursion-available field is set in a reply if the DNS server supports recursion. In the header, there are also four number-of fields. These fields indicate the
number of occurrences of the four types of data sections that follow the header.
• The question section contains information about the query that is being made.
This section includes (1) a name field that contains the name that is being queried, and (2) a type field that indicates the type of question being asked about the
name—for example, a host address associated with a name (Type A) or the mail
server for a name (Type MX).

Identification

Flags

Number of questions

Number of answer RRs

Number of authority RRs

Number of additional RRs

Questions
(variable number of questions)

12 bytes

Name, type fields for
a query

Answers
(variable number of resource records)

RRs in response to query

Authority
(variable number of resource records)

Records for
authoritative servers

Additional information
(variable number of resource records)

Additional “helpful”
info that may be used

Figure 2.21 ♦ DNS message format

133

134

CHAPTER 2

•

APPLICATION LAYER

• In a reply from a DNS server, the answer section contains the resource records for
the name that was originally queried. Recall that in each resource record there is the
Type (for example, A, NS, CNAME, and MX), the Value, and the TTL. A reply can
return multiple RRs in the answer, since a hostname can have multiple IP addresses
(for example, for replicated Web servers, as discussed earlier in this section).
• The authority section contains records of other authoritative servers.
• The additional section contains other helpful records. For example, the answer
field in a reply to an MX query contains a resource record providing the canonical hostname of a mail server. The additional section contains a Type A record
providing the IP address for the canonical hostname of the mail server.
How would you like to send a DNS query message directly from the host you’re
working on to some DNS server? This can easily be done with the nslookup program,
which is available from most Windows and UNIX platforms. For example, from a Windows host, open the Command Prompt and invoke the nslookup program by simply typing “nslookup.” After invoking nslookup, you can send a DNS query to any DNS server
(root, TLD, or authoritative). After receiving the reply message from the DNS server,
nslookup will display the records included in the reply (in a human-readable format). As
an alternative to running nslookup from your own host, you can visit one of many Web
sites that allow you to remotely employ nslookup. (Just type “nslookup” into a search
engine and you’ll be brought to one of these sites.) The DNS Wireshark lab at the end of
this chapter will allow you to explore the DNS in much more detail.

Inserting Records into the DNS Database
The discussion above focused on how records are retrieved from the DNS database.
You might be wondering how records get into the database in the first place. Let’s look
at how this is done in the context of a specific example. Suppose you have just created
an exciting new startup company called Network Utopia. The first thing you’ll surely
want to do is register the domain name networkutopia.com at a registrar. A registrar is a commercial entity that verifies the uniqueness of the domain name, enters
the domain name into the DNS database (as discussed below), and collects a small fee
from you for its services. Prior to 1999, a single registrar, Network Solutions, had a
monopoly on domain name registration for com, net, and org domains. But now
there are many registrars competing for customers, and the Internet Corporation for
Assigned Names and Numbers (ICANN) accredits the various registrars. A complete
list of accredited registrars is available at http://www.internic.net.
When you register the domain name networkutopia.com with some registrar, you also need to provide the registrar with the names and IP addresses of
your primary and secondary authoritative DNS servers. Suppose the names and IP
addresses are dns1.networkutopia.com, dns2.networkutopia.com,
212.2.212.1, and 212.212.212.2. For each of these two authoritative DNS

2.4

•

DNS—THE INTERNET’S DIRECTORY SERVICE

FOCUS ON SECURITY
DNS VU LN ERABILIT IES
We have seen that DNS is a critical component of the Internet infrastructure, with
many important services—including the Web and e-mail—simply incapable of functioning without it. We therefore naturally ask, how can DNS be attacked? Is DNS a
sitting duck, waiting to be knocked out of service, while taking most Internet applications down with it?
The first type of attack that comes to mind is a DDoS bandwidth-flooding attack
(see Section 1.6) against DNS servers. For example, an attacker could attempt to
send to each DNS root server a deluge of packets, so many that the majority of
legitimate DNS queries never get answered. Such a large-scale DDoS attack against
DNS root servers actually took place on October 21, 2002. In this attack, the attackers leveraged a botnet to send truck loads of ICMP ping messages to each of the
13 DNS root IP addresses. (ICMP messages are discussed in Section 5.6. For now,
it suffices to know that ICMP packets are special types of IP datagrams.) Fortunately,
this large-scale attack caused minimal damage, having little or no impact on users’
Internet experience. The attackers did succeed at directing a deluge of packets at the
root servers. But many of the DNS root servers were protected by packet filters, configured to always block all ICMP ping messages directed at the root servers. These
protected servers were thus spared and functioned as normal. Furthermore, most local
DNS servers cache the IP addresses of top-level-domain servers, allowing the query
process to often bypass the DNS root servers.
A potentially more effective DDoS attack against DNS is send a deluge of DNS
queries to top-level-domain servers, for example, to top-level-domain servers that
handle the .com domain. It is harder to filter DNS queries directed to DNS servers;
and top-level-domain servers are not as easily bypassed as are root servers. Such an
attack took place against the top-level-domain service provider Dyn on October 21,
2016. This DDoS attack was accomplished through a large number of DNS lookup
requests from a botnet consisting of about one hundred thousand IoT devices such as
printers, IP cameras, residential gateways and baby monitors that had been infected
with Mirai malware. For almost a full day, Amazon, Twitter, Netflix, Github and
Spotify were disturbed.
DNS could potentially be attacked in other ways. In a man-in-the-middle attack,
the attacker intercepts queries from hosts and returns bogus replies. In the DNS poisoning attack, the attacker sends bogus replies to a DNS server, tricking the server
into accepting bogus records into its cache. Either of these attacks could be used,
for example, to redirect an unsuspecting Web user to the attacker’s Web site. The
DNS Security Extensions (DNSSEC [Gieben 2004; RFC 4033] have been designed
and deployed to protect against such exploits. DNSSEC, a secured version of DNS,
addresses many of these possible attacks and is gaining popularity in the Internet.

135

136

CHAPTER 2

•

APPLICATION LAYER

servers, the registrar would then make sure that a Type NS and a Type A record are
entered into the TLD com servers. Specifically, for the primary authoritative server
for networkutopia.com, the registrar would insert the following two resource
records into the DNS system:
(networkutopia.com, dns1.networkutopia.com, NS)
(dns1.networkutopia.com, 212.212.212.1, A)
You’ll also have to make sure that the Type A resource record for your Web server
www.networkutopia.com and the Type MX resource record for your mail
server mail.networkutopia.com are entered into your authoritative DNS
servers. (Until recently, the contents of each DNS server were configured statically,
for example, from a configuration file created by a system manager. More recently,
an UPDATE option has been added to the DNS protocol to allow data to be dynamically added or deleted from the database via DNS messages. [RFC 2136] and [RFC
3007] specify DNS dynamic updates.)
Once all of these steps are completed, people will be able to visit your Web site
and send e-mail to the employees at your company. Let’s conclude our discussion of
DNS by verifying that this statement is true. This verification also helps to solidify
what we have learned about DNS. Suppose Alice in Australia wants to view the Web
page www.networkutopia.com. As discussed earlier, her host will first send a
DNS query to her local DNS server. The local DNS server will then contact a TLD
com server. (The local DNS server will also have to contact a root DNS server if the
address of a TLD com server is not cached.) This TLD server contains the Type NS
and Type A resource records listed above, because the registrar had these resource
records inserted into all of the TLD com servers. The TLD com server sends a reply
to Alice’s local DNS server, with the reply containing the two resource records. The
local DNS server then sends a DNS query to 212.212.212.1, asking for the Type
A record corresponding to www.networkutopia.com. This record provides the
IP address of the desired Web server, say, 212.212.71.4, which the local DNS
server passes back to Alice’s host. Alice’s browser can now initiate a TCP connection to the host 212.212.71.4 and send an HTTP request over the connection.
Whew! There’s a lot more going on than what meets the eye when one surfs the Web!

2.5 Peer-to-Peer File Distribution
The applications described in this chapter thus far—including the Web, e-mail, and
DNS—all employ client-server architectures with significant reliance on always-on
infrastructure servers. Recall from Section 2.1.1 that with a P2P architecture, there
is minimal (or no) reliance on always-on infrastructure servers. Instead, pairs of
intermittently connected hosts, called peers, communicate directly with each other.
The peers are not owned by a service provider, but are instead PCs, laptops, and
smartpones controlled by users.

2.5

•

PEER-TO-PEER FILE DISTRIBUTION

In this section, we consider a very natural P2P application, namely, distributing a
large file from a single server to a large number of hosts (called peers). The file might
be a new version of the Linux operating system, a software patch for an existing
operating system or an MPEG video file. In client-server file distribution, the server
must send a copy of the file to each of the peers—placing an enormous burden on the
server and consuming a large amount of server bandwidth. In P2P file distribution,
each peer can redistribute any portion of the file it has received to any other peers,
thereby assisting the server in the distribution process. As of 2020, the most popular
P2P file distribution protocol is BitTorrent. Originally developed by Bram Cohen,
there are now many different independent BitTorrent clients conforming to the BitTorrent protocol, just as there are a number of Web browser clients that conform to
the HTTP protocol. In this subsection, we first examine the self-scalability of P2P
architectures in the context of file distribution. We then describe BitTorrent in some
detail, highlighting its most important characteristics and features.

Scalability of P2P Architectures
To compare client-server architectures with peer-to-peer architectures, and illustrate
the inherent self-scalability of P2P, we now consider a simple quantitative model
for distributing a file to a fixed set of peers for both architecture types. As shown
in Figure 2.22, the server and the peers are connected to the Internet with access

File: F
Server
u1

d1

u2

us

d2
u3
d3

dN

Internet

uN

u4
d4
d6

u6

d5

u5

Figure 2.22 ♦ An illustrative file distribution problem

137

138

CHAPTER 2

•

APPLICATION LAYER

links. Denote the upload rate of the server’s access link by us, the upload rate of the
ith peer’s access link by ui, and the download rate of the ith peer’s access link by
di. Also denote the size of the file to be distributed (in bits) by F and the number of
peers that want to obtain a copy of the file by N. The distribution time is the time it
takes to get a copy of the file to all N peers. In our analysis of the distribution time
below, for both client-server and P2P architectures, we make the simplifying (and
generally accurate [Akella 2003]) assumption that the Internet core has abundant
bandwidth, implying that all of the bottlenecks are in access networks. We also suppose that the server and clients are not participating in any other network applications, so that all of their upload and download access bandwidth can be fully devoted
to distributing this file.
Let’s first determine the distribution time for the client-server architecture,
which we denote by Dcs. In the client-server architecture, none of the peers aids in
distributing the file. We make the following observations:
• The server must transmit one copy of the file to each of the N peers. Thus, the
server must transmit NF bits. Since the server’s upload rate is us, the time to distribute the file must be at least NF/us.
• Let dmin denote the download rate of the peer with the lowest download rate, that
is, dmin = min5 d1, dp, . . . , dN 6 . The peer with the lowest download rate cannot
obtain all F bits of the file in less than F/dmin seconds. Thus, the minimum distribution time is at least F/dmin.
Putting these two observations together, we obtain

Dcs Ú maxb

NF F
,
r.
us dmin

This provides a lower bound on the minimum distribution time for the client-server
architecture. In the homework problems, you will be asked to show that the server
can schedule its transmissions so that the lower bound is actually achieved. So let’s
take this lower bound provided above as the actual distribution time, that is,
Dcs = maxb

NF F
,
r
us dmin

(2.1)

We see from Equation 2.1 that for N large enough, the client-server distribution time
is given by NF/us. Thus, the distribution time increases linearly with the number of
peers N. So, for example, if the number of peers from one week to the next increases
a thousand-fold from a thousand to a million, the time required to distribute the file
to all peers increases by 1,000.

2.5

•

PEER-TO-PEER FILE DISTRIBUTION

Let’s now go through a similar analysis for the P2P architecture, where each peer
can assist the server in distributing the file. In particular, when a peer receives some
file data, it can use its own upload capacity to redistribute the data to other peers. Calculating the distribution time for the P2P architecture is somewhat more complicated
than for the client-server architecture, since the distribution time depends on how
each peer distributes portions of the file to the other peers. Nevertheless, a simple
expression for the minimal distribution time can be obtained [Kumar 2006]. To this
end, we first make the following observations:
• At the beginning of the distribution, only the server has the file. To get this file
into the community of peers, the server must send each bit of the file at least once
into its access link. Thus, the minimum distribution time is at least F/us. (Unlike
the client-server scheme, a bit sent once by the server may not have to be sent by
the server again, as the peers may redistribute the bit among themselves.)
• As with the client-server architecture, the peer with the lowest download rate
cannot obtain all F bits of the file in less than F/dmin seconds. Thus, the minimum
distribution time is at least F/dmin.
• Finally, observe that the total upload capacity of the system as a whole is equal
to the upload rate of the server plus the upload rates of each of the individual
peers, that is, utotal = us + u1 + g + uN. The system must deliver (upload) F
bits to each of the N peers, thus delivering a total of NF bits. This cannot be done
at a rate faster than utotal. Thus, the minimum distribution time is also at least
NF/(us + u1 + g + uN).
Putting these three observations together, we obtain the minimum distribution
time for P2P, denoted by DP2P.
F F
,
,
DP2P Ú max c us dmin

NF

us + a ui
N

s

(2.2)

i=1

Equation 2.2 provides a lower bound for the minimum distribution time for the P2P
architecture. It turns out that if we imagine that each peer can redistribute a bit as
soon as it receives the bit, then there is a redistribution scheme that actually achieves
this lower bound [Kumar 2006]. (We will prove a special case of this result in the
homework.) In reality, where chunks of the file are redistributed rather than individual bits, Equation 2.2 serves as a good approximation of the actual minimum
distribution time. Thus, let’s take the lower bound provided by Equation 2.2 as the
actual minimum distribution time, that is,
F F
,
,
DP2P = max c us dmin

NF

us + a ui
N

i=1

s

(2.3)

139

CHAPTER 2

•

APPLICATION LAYER

3.5
3.0
Minimum distribution time

140

Client-Server
2.5
2.0
1.5
P2P

1.0
0.5
0
0

5

10

15

20

25

30

35

N

Figure 2.23 ♦ Distribution time for P2P and client-server architectures
Figure 2.23 compares the minimum distribution time for the client-server and
P2P architectures assuming that all peers have the same upload rate u. In Figure 2.23,
we have set F/u = 1 hour, us = 10u, and dmin Ú us. Thus, a peer can transmit the
entire file in one hour, the server transmission rate is 10 times the peer upload rate,
and (for simplicity) the peer download rates are set large enough so as not to have
an effect. We see from Figure 2.23 that for the client-server architecture, the distribution time increases linearly and without bound as the number of peers increases.
However, for the P2P architecture, the minimal distribution time is not only always
less than the distribution time of the client-server architecture; it is also less than one
hour for any number of peers N. Thus, applications with the P2P architecture can be
self-scaling. This scalability is a direct consequence of peers being redistributors as
well as consumers of bits.

BitTorrent
BitTorrent is a popular P2P protocol for file distribution [Chao 2011]. In BitTorrent
lingo, the collection of all peers participating in the distribution of a particular file is
called a torrent. Peers in a torrent download equal-size chunks of the file from one
another, with a typical chunk size of 256 KBytes. When a peer first joins a torrent, it
has no chunks. Over time it accumulates more and more chunks. While it downloads
chunks it also uploads chunks to other peers. Once a peer has acquired the entire
file, it may (selfishly) leave the torrent, or (altruistically) remain in the torrent and
continue to upload chunks to other peers. Also, any peer may leave the torrent at any
time with only a subset of chunks, and later rejoin the torrent.

2.5

Tracker

•

PEER-TO-PEER FILE DISTRIBUTION

Peer

Obtain
list of
peers

Trading chunks
Alice

Figure 2.24 ♦ File distribution with BitTorrent
Let’s now take a closer look at how BitTorrent operates. Since BitTorrent is
a rather complicated protocol and system, we’ll only describe its most important
mechanisms, sweeping some of the details under the rug; this will allow us to see
the forest through the trees. Each torrent has an infrastructure node called a tracker.
When a peer joins a torrent, it registers itself with the tracker and periodically informs
the tracker that it is still in the torrent. In this manner, the tracker keeps track of the
peers that are participating in the torrent. A given torrent may have fewer than ten or
more than a thousand peers participating at any instant of time.
As shown in Figure 2.24, when a new peer, Alice, joins the torrent, the tracker
randomly selects a subset of peers (for concreteness, say 50) from the set of participating peers, and sends the IP addresses of these 50 peers to Alice. Possessing this
list of peers, Alice attempts to establish concurrent TCP connections with all the
peers on this list. Let’s call all the peers with which Alice succeeds in establishing a
TCP connection “neighboring peers.” (In Figure 2.24, Alice is shown to have only
three neighboring peers. Normally, she would have many more.) As time evolves,
some of these peers may leave and other peers (outside the initial 50) may attempt to
establish TCP connections with Alice. So a peer’s neighboring peers will fluctuate
over time.

141

142

CHAPTER 2

•

APPLICATION LAYER

At any given time, each peer will have a subset of chunks from the file, with different peers having different subsets. Periodically, Alice will ask each of her neighboring peers (over the TCP connections) for the list of the chunks they have. If Alice
has L different neighbors, she will obtain L lists of chunks. With this knowledge,
Alice will issue requests (again over the TCP connections) for chunks she currently
does not have.
So at any given instant of time, Alice will have a subset of chunks and will know
which chunks her neighbors have. With this information, Alice will have two important decisions to make. First, which chunks should she request first from her neighbors? And second, to which of her neighbors should she send requested chunks? In
deciding which chunks to request, Alice uses a technique called rarest first. The
idea is to determine, from among the chunks she does not have, the chunks that are
the rarest among her neighbors (that is, the chunks that have the fewest repeated copies among her neighbors) and then request those rarest chunks first. In this manner,
the rarest chunks get more quickly redistributed, aiming to (roughly) equalize the
numbers of copies of each chunk in the torrent.
To determine which requests she responds to, BitTorrent uses a clever trading
algorithm. The basic idea is that Alice gives priority to the neighbors that are currently supplying her data at the highest rate. Specifically, for each of her neighbors,
Alice continually measures the rate at which she receives bits and determines the four
peers that are feeding her bits at the highest rate. She then reciprocates by sending
chunks to these same four peers. Every 10 seconds, she recalculates the rates and possibly modifies the set of four peers. In BitTorrent lingo, these four peers are said to be
unchoked. Importantly, every 30 seconds, she also picks one additional neighbor at
random and sends it chunks. Let’s call the randomly chosen peer Bob. In BitTorrent
lingo, Bob is said to be optimistically unchoked. Because Alice is sending data to
Bob, she may become one of Bob’s top four uploaders, in which case Bob would start
to send data to Alice. If the rate at which Bob sends data to Alice is high enough, Bob
could then, in turn, become one of Alice’s top four uploaders. In other words, every
30 seconds, Alice will randomly choose a new trading partner and initiate trading
with that partner. If the two peers are satisfied with the trading, they will put each
other in their top four lists and continue trading with each other until one of the peers
finds a better partner. The effect is that peers capable of uploading at compatible
rates tend to find each other. The random neighbor selection also allows new peers
to get chunks, so that they can have something to trade. All other neighboring peers
besides these five peers (four “top” peers and one probing peer) are “choked,” that
is, they do not receive any chunks from Alice. BitTorrent has a number of interesting
mechanisms that are not discussed here, including pieces (mini-chunks), pipelining,
random first selection, endgame mode, and anti-snubbing [Cohen 2003].
The incentive mechanism for trading just described is often referred to as tit-fortat [Cohen 2003]. It has been shown that this incentive scheme can be circumvented
[Liogkas 2006; Locher 2006; Piatek 2008]. Nevertheless, the BitTorrent ecosystem
is wildly successful, with millions of simultaneous peers actively sharing files in

2.6

•

VIDEO STREAMING AND CONTENT DISTRIBUTION NETWORKS

hundreds of thousands of torrents. If BitTorrent had been designed without tit-for-tat
(or a variant), but otherwise exactly the same, BitTorrent would likely not even exist
now, as the majority of the users would have been freeriders [Saroiu 2002].
We close our discussion on P2P by briefly mentioning another application of P2P,
namely, Distributed Hast Table (DHT). A distributed hash table is a simple database,
with the database records being distributed over the peers in a P2P system. DHTs have
been widely implemented (e.g., in BitTorrent) and have been the subject of extensive
research. An overview is provided in a Video Note in the companion website.

2.6 Video Streaming and Content Distribution
Networks

By many estimates, streaming video—including Netflix, YouTube and Amazon
Prime—account for about 80% of Internet traffic in 2020 [Cisco 2020]. This section
we will provide an overview of how popular video streaming services are implemented in today’s Internet. We will see they are implemented using application-level
protocols and servers that function in some ways like a cache.

2.6.1 Internet Video
In streaming stored video applications, the underlying medium is prerecorded video,
such as a movie, a television show, a prerecorded sporting event, or a prerecorded
user-generated video (such as those commonly seen on YouTube). These prerecorded videos are placed on servers, and users send requests to the servers to view
the videos on demand. Many Internet companies today provide streaming video,
including, Netflix, YouTube (Google), Amazon, and TikTok.
But before launching into a discussion of video streaming, we should first get
a quick feel for the video medium itself. A video is a sequence of images, typically being displayed at a constant rate, for example, at 24 or 30 images per second.
An uncompressed, digitally encoded image consists of an array of pixels, with each
pixel encoded into a number of bits to represent luminance and color. An important
characteristic of video is that it can be compressed, thereby trading off video quality
with bit rate. Today’s off-the-shelf compression algorithms can compress a video to
essentially any bit rate desired. Of course, the higher the bit rate, the better the image
quality and the better the overall user viewing experience.
From a networking perspective, perhaps the most salient characteristic of video
is its high bit rate. Compressed Internet video typically ranges from 100 kbps for
low-quality video to over 4 Mbps for streaming high-definition movies; 4K streaming envisions a bitrate of more than 10 Mbps. This can translate to huge amount of
traffic and storage, particularly for high-end video. For example, a single 2 Mbps

143

VideoNote
Walking though
distributed hash tables

144

CHAPTER 2

•

APPLICATION LAYER

video with a duration of 67 minutes will consume 1 gigabyte of storage and traffic.
By far, the most important performance measure for streaming video is average endto-end throughput. In order to provide continuous playout, the network must provide
an average throughput to the streaming application that is at least as large as the bit
rate of the compressed video.
We can also use compression to create multiple versions of the same video, each
at a different quality level. For example, we can use compression to create, say, three
versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users can then
decide which version they want to watch as a function of their current available bandwidth. Users with high-speed Internet connections might choose the 3 Mbps version;
users watching the video over 3G with a smartphone might choose the 300 kbps version.

2.6.2 HTTP Streaming and DASH
In HTTP streaming, the video is simply stored at an HTTP server as an ordinary
file with a specific URL. When a user wants to see the video, the client establishes
a TCP connection with the server and issues an HTTP GET request for that URL.
The server then sends the video file, within an HTTP response message, as quickly
as the underlying network protocols and traffic conditions will allow. On the client
side, the bytes are collected in a client application buffer. Once the number of bytes
in this buffer exceeds a predetermined threshold, the client application begins playback—specifically, the streaming video application periodically grabs video frames
from the client application buffer, decompresses the frames, and displays them on
the user’s screen. Thus, the video streaming application is displaying video as it is
receiving and buffering frames corresponding to latter parts of the video.
Although HTTP streaming, as described in the previous paragraph, has been
extensively deployed in practice (for example, by YouTube since its inception), it has
a major shortcoming: All clients receive the same encoding of the video, despite the
large variations in the amount of bandwidth available to a client, both across different
clients and also over time for the same client. This has led to the development of a new
type of HTTP-based streaming, often referred to as Dynamic Adaptive Streaming
over HTTP (DASH). In DASH, the video is encoded into several different versions,
with each version having a different bit rate and, correspondingly, a different quality
level. The client dynamically requests chunks of video segments of a few seconds in
length. When the amount of available bandwidth is high, the client naturally selects
chunks from a high-rate version; and when the available bandwidth is low, it naturally
selects from a low-rate version. The client selects different chunks one at a time with
HTTP GET request messages [Akhshabi 2011].
DASH allows clients with different Internet access rates to stream in video at
different encoding rates. Clients with low-speed 3G connections can receive a low
bit-rate (and low-quality) version, and clients with fiber connections can receive a
high-quality version. DASH also allows a client to adapt to the available bandwidth
if the available end-to-end bandwidth changes during the session. This feature is

2.6

•

VIDEO STREAMING AND CONTENT DISTRIBUTION NETWORKS

particularly important for mobile users, who typically see their bandwidth availability fluctuate as they move with respect to the base stations.
With DASH, each video version is stored in the HTTP server, each with a different URL. The HTTP server also has a manifest file, which provides a URL for each
version along with its bit rate. The client first requests the manifest file and learns
about the various versions. The client then selects one chunk at a time by specifying a
URL and a byte range in an HTTP GET request message for each chunk. While downloading chunks, the client also measures the received bandwidth and runs a rate determination algorithm to select the chunk to request next. Naturally, if the client has a lot
of video buffered and if the measured receive bandwidth is high, it will choose a chunk
from a high-bitrate version. And naturally if the client has little video buffered and the
measured received bandwidth is low, it will choose a chunk from a low-bitrate version.
DASH therefore allows the client to freely switch among different quality levels.

2.6.3 Content Distribution Networks
Today, many Internet video companies are distributing on-demand multi-Mbps
streams to millions of users on a daily basis. YouTube, for example, with a library
of hundreds of millions of videos, distributes hundreds of millions of video streams
to users around the world every day. Streaming all this traffic to locations all over
the world while providing continuous playout and high interactivity is clearly a challenging task.
For an Internet video company, perhaps the most straightforward approach to
providing streaming video service is to build a single massive data center, store all
of its videos in the data center, and stream the videos directly from the data center
to clients worldwide. But there are three major problems with this approach. First, if
the client is far from the data center, server-to-client packets will cross many communication links and likely pass through many ISPs, with some of the ISPs possibly
located on different continents. If one of these links provides a throughput that is less
than the video consumption rate, the end-to-end throughput will also be below the
consumption rate, resulting in annoying freezing delays for the user. (Recall from
Chapter 1 that the end-to-end throughput of a stream is governed by the throughput
at the bottleneck link.) The likelihood of this happening increases as the number of
links in the end-to-end path increases. A second drawback is that a popular video will
likely be sent many times over the same communication links. Not only does this
waste network bandwidth, but the Internet video company itself will be paying its
provider ISP (connected to the data center) for sending the same bytes into the Internet over and over again. A third problem with this solution is that a single data center
represents a single point of failure—if the data center or its links to the Internet goes
down, it would not be able to distribute any video streams.
In order to meet the challenge of distributing massive amounts of video data
to users distributed around the world, almost all major video-streaming companies
make use of Content Distribution Networks (CDNs). A CDN manages servers in

145

146

CHAPTER 2

•

APPLICATION LAYER

multiple geographically distributed locations, stores copies of the videos (and other
types of Web content, including documents, images, and audio) in its servers, and
attempts to direct each user request to a CDN location that will provide the best user
experience. The CDN may be a private CDN, that is, owned by the content provider
itself; for example, Google’s CDN distributes YouTube videos and other types of
content. The CDN may alternatively be a third-party CDN that distributes content
on behalf of multiple content providers; Akamai, Limelight and Level-3 all operate
third-party CDNs. A very readable overview of modern CDNs is [Leighton 2009;
Nygren 2010].
CDNs typically adopt one of two different server placement philosophies
[Huang 2008]:
• Enter Deep. One philosophy, pioneered by Akamai, is to enter deep into the
access networks of Internet Service Providers, by deploying server clusters in
access ISPs all over the world. (Access networks are described in Section 1.3.)
Akamai takes this approach with clusters in thousands of locations. The goal is
to get close to end users, thereby improving user-perceived delay and throughput
by decreasing the number of links and routers between the end user and the CDN
server from which it receives content. Because of this highly distributed design,
the task of maintaining and managing the clusters becomes challenging.
• Bring Home. A second design philosophy, taken by Limelight and many
other CDN companies, is to bring the ISPs home by building large clusters
at a smaller number (for example, tens) of sites. Instead of getting inside the
access ISPs, these CDNs typically place their clusters in Internet Exchange
Points (IXPs) (see Section 1.3). Compared with the enter-deep design philosophy, the bring-home design typically results in lower maintenance and
management overhead, possibly at the expense of higher delay and lower
throughput to end users.
Once its clusters are in place, the CDN replicates content across its clusters. The
CDN may not want to place a copy of every video in each cluster, since some videos
are rarely viewed or are only popular in some countries. In fact, many CDNs do not
push videos to their clusters but instead use a simple pull strategy: If a client requests
a video from a cluster that is not storing the video, then the cluster retrieves the
video (from a central repository or from another cluster) and stores a copy locally
while streaming the video to the client at the same time. Similar Web caching (see
Section 2.2.5), when a cluster’s storage becomes full, it removes videos that are not
frequently requested.

CDN Operation
Having identified the two major approaches toward deploying a CDN, let’s now dive
down into the nuts and bolts of how a CDN operates. When a browser in a user’s

2.6

•

VIDEO STREAMING AND CONTENT DISTRIBUTION NETWORKS

CASE STUDY
G OOGLE’ S NET W O RK INF RAST R UC TUR E
To support its vast array of services—including search, Gmail, calendar, YouTube
video, maps, documents, and social networks—Google has deployed an extensive
private network and CDN infrastructure. Google’s CDN infrastructure has three tiers
of server clusters:

•

Nineteen “mega data centers” in North America, Europe, and Asia [Google
Locations 2020], with each data center having on the order of 100,000 servers.
These mega data centers are responsible for serving dynamic (and often personalized) content, including search results and Gmail messages.

•

With about 90 clusters in IXPs scattered throughout the world, with each cluster
consisting of hundreds of servers servers [Adhikari 2011a] [Google CDN 2020].
These clusters are responsible for serving static content, including YouTube videos.

•

Many hundreds of “enter-deep” clusters located within an access ISP. Here a cluster
typically consists of tens of servers within a single rack. These enter-deep servers
perform TCP splitting (see Section 3.7) and serve static content [Chen 2011],
including the static portions of Web pages that embody search results.

All of these data centers and cluster locations are networked together with
Google’s own private network. When a user makes a search query, often the query
is first sent over the local ISP to a nearby enter-deep cache, from where the static
content is retrieved; while providing the static content to the client, the nearby cache
also forwards the query over Google’s private network to one of the mega data centers, from where the personalized search results are retrieved. For a YouTube video,
the video itself may come from one of the bring-home caches, whereas portions of
the Web page surrounding the video may come from the nearby enter-deep cache,
and the advertisements surrounding the video come from the data centers. In summary, except for the local ISPs, the Google cloud services are largely provided by a
network infrastructure that is independent of the public Internet.

host is instructed to retrieve a specific video (identified by a URL), the CDN must
intercept the request so that it can (1) determine a suitable CDN server cluster for that
client at that time, and (2) redirect the client’s request to a server in that cluster. We’ll
shortly discuss how a CDN can determine a suitable cluster. But first let’s examine
the mechanics behind intercepting and redirecting a request.
Most CDNs take advantage of DNS to intercept and redirect requests; an interesting discussion of such a use of the DNS is [Vixie 2009]. Let’s consider a simple

147

148

CHAPTER 2

•

APPLICATION LAYER

example to illustrate how the DNS is typically involved. Suppose a content provider,
NetCinema, employs the third-party CDN company, KingCDN, to distribute its videos to its customers. On the NetCinema Web pages, each of its videos is assigned a
URL that includes the string “video” and a unique identifier for the video itself; for
example, Transformers 7 might be assigned http://video.netcinema.com/6Y7B23V.
Six steps then occur, as shown in Figure 2.25:
1. The user visits the Web page at NetCinema.
2. When the user clicks on the link http://video.netcinema.com/6Y7B23V, the
user’s host sends a DNS query for video.netcinema.com.
3. The user’s Local DNS Server (LDNS) relays the DNS query to an authoritative
DNS server for NetCinema, which observes the string “video” in the hostname video.netcinema.com. To “hand over” the DNS query to KingCDN,
instead of returning an IP address, the NetCinema authoritative DNS server
returns to the LDNS a hostname in the KingCDN’s domain, for example,
a1105.kingcdn.com.
4. From this point on, the DNS query enters into KingCDN’s private DNS infrastructure. The user’s LDNS then sends a second query, now for a1105.kingcdn.
com, and KingCDN’s DNS system eventually returns the IP addresses of a
KingCDN content server to the LDNS. It is thus here, within the KingCDN’s
DNS system, that the CDN server from which the client will receive its content
is specified.

1

www.NetCinema.com

3

2
5

6

Local
DNS server

KingCDN content
distribution server

4

NetCinema authoritative
DNS server

KingCDN authoritative
server

Figure 2.25 ♦ DNS redirects a user’s request to a CDN server

2.6

•

VIDEO STREAMING AND CONTENT DISTRIBUTION NETWORKS

5. The LDNS forwards the IP address of the content-serving CDN node to the
user’s host.
6. Once the client receives the IP address for a KingCDN content server, it establishes a direct TCP connection with the server at that IP address and issues an
HTTP GET request for the video. If DASH is used, the server will first send to
the client a manifest file with a list of URLs, one for each version of the video,
and the client will dynamically select chunks from the different versions.

Cluster Selection Strategies
At the core of any CDN deployment is a cluster selection strategy, that is, a mechanism for dynamically directing clients to a server cluster or a data center within the
CDN. As we just saw, the CDN learns the IP address of the client’s LDNS server
via the client’s DNS lookup. After learning this IP address, the CDN needs to select
an appropriate cluster based on this IP address. CDNs generally employ proprietary
cluster selection strategies. We now briefly survey a few approaches, each of which
has its own advantages and disadvantages.
One simple strategy is to assign the client to the cluster that is geographically closest. Using commercial geo-location databases (such as Quova [Quova 2020] and MaxMind [MaxMind 2020]), each LDNS IP address is mapped to a geographic location.
When a DNS request is received from a particular LDNS, the CDN chooses the geographically closest cluster, that is, the cluster that is the fewest kilometers from the LDNS
“as the bird flies.” Such a solution can work reasonably well for a large fraction of the clients [Agarwal 2009]. However, for some clients, the solution may perform poorly, since
the geographically closest cluster may not be the closest cluster in terms of the length
or number of hops of the network path. Furthermore, a problem inherent with all DNSbased approaches is that some end-users are configured to use remotely located LDNSs
[Shaikh 2001; Mao 2002], in which case the LDNS location may be far from the client’s
location. Moreover, this simple strategy ignores the variation in delay and available bandwidth over time of Internet paths, always assigning the same cluster to a particular client.
In order to determine the best cluster for a client based on the current traffic
conditions, CDNs can instead perform periodic real-time measurements of delay
and loss performance between their clusters and clients. For instance, a CDN can
have each of its clusters periodically send probes (for example, ping messages or
DNS queries) to all of the LDNSs around the world. One drawback of this approach
is that many LDNSs are configured to not respond to such probes.

2.6.4 Case Studies: Netflix and YouTube
We conclude our discussion of streaming stored video by taking a look at two highly
successful large-scale deployments: Netflix and YouTube. We’ll see that each of
these systems take a very different approach, yet employ many of the underlying
principles discussed in this section.

149

150

CHAPTER 2

•

APPLICATION LAYER

Netflix
As of 2020, Netflix is the leading service provider for online movies and TV series in
North America. As we discuss below, Netflix video distribution has two major components: the Amazon cloud and its own private CDN infrastructure.
Netflix has a Web site that handles numerous functions, including user registration and login, billing, movie catalogue for browsing and searching, and a movie
recommendation system. As shown in Figure 2.26, this Web site (and its associated
backend databases) run entirely on Amazon servers in the Amazon cloud. Additionally, the Amazon cloud handles the following critical functions:
• Content ingestion. Before Netflix can distribute a movie to its customers, it must
first ingest and process the movie. Netflix receives studio master versions of
movies and uploads them to hosts in the Amazon cloud.
• Content processing. The machines in the Amazon cloud create many different
formats for each movie, suitable for a diverse array of client video players running on desktop computers, smartphones, and game consoles connected to televisions. A different version is created for each of these formats and at multiple bit
rates, allowing for adaptive streaming over HTTP using DASH.
• Uploading versions to its CDN. Once all of the versions of a movie have been
created, the hosts in the Amazon cloud upload the versions to its CDN.

Amazon Cloud

Upload
versions
to CDNs

CDN server
Manifest
file

Video
chunks
(DASH)

CDN server

Client

Figure 2.26 ♦ Netflix video streaming platform

CDN server

2.6

•

VIDEO STREAMING AND CONTENT DISTRIBUTION NETWORKS

When Netflix first rolled out its video streaming service in 2007, it employed
three third-party CDN companies to distribute its video content. Netflix has since
created its own private CDN, from which it now streams all of its videos. To create
its own CDN, Netflix has installed server racks both in IXPs and within residential ISPs themselves. Netflix currently has server racks in over 200 IXP locations;
see [Bottger 2018] [Netflix Open Connect 2020] for a current list of IXPs housing
Netflix racks. There are also hundreds of ISP locations housing Netflix racks; also
see [Netflix Open Connect 2020], where Netflix provides to potential ISP partners
instructions about installing a (free) Netflix rack for their networks. Each server in
the rack has several 10 Gbps Ethernet ports and over 100 terabytes of storage. The
number of servers in a rack varies: IXP installations often have tens of servers and
contain the entire Netflix streaming video library, including multiple versions of the
videos to support DASH. Netflix does not use pull-caching (Section 2.2.5) to populate its CDN servers in the IXPs and ISPs. Instead, Netflix distributes by pushing the
videos to its CDN servers during off-peak hours. For those locations that cannot hold
the entire library, Netflix pushes only the most popular videos, which are determined
on a day-to-day basis. The Netflix CDN design is described in some detail in the
YouTube videos [Netflix Video 1] and [Netflix Video 2]; see also [Bottger 2018].
Having described the components of the Netflix architecture, let’s take a closer
look at the interaction between the client and the various servers that are involved in
movie delivery. As indicated earlier, the Web pages for browsing the Netflix video
library are served from servers in the Amazon cloud. When a user selects a movie to
play, the Netflix software, running in the Amazon cloud, first determines which of
its CDN servers have copies of the movie. Among the servers that have the movie,
the software then determines the “best” server for that client request. If the client is
using a residential ISP that has a Netflix CDN server rack installed in that ISP, and
this rack has a copy of the requested movie, then a server in this rack is typically
selected. If not, a server at a nearby IXP is typically selected.
Once Netflix determines the CDN server that is to deliver the content, it sends
the client the IP address of the specific server as well as a manifest file, which has
the URLs for the different versions of the requested movie. The client and that CDN
server then directly interact using a proprietary version of DASH. Specifically,
as described in Section 2.6.2, the client uses the byte-range header in HTTP GET
request messages, to request chunks from the different versions of the movie. Netflix
uses chunks that are approximately four-seconds long [Adhikari 2012]. While the
chunks are being downloaded, the client measures the received throughput and runs
a rate-determination algorithm to determine the quality of the next chunk to request.
Netflix embodies many of the key principles discussed earlier in this section,
including adaptive streaming and CDN distribution. However, because Netflix uses
its own private CDN, which distributes only video (and not Web pages), Netflix has
been able to simplify and tailor its CDN design. In particular, Netflix does not need to
employ DNS redirect, as discussed in Section 2.6.3, to connect a particular client to a
CDN server; instead, the Netflix software (running in the Amazon cloud) directly tells

151

152

CHAPTER 2

•

APPLICATION LAYER

the client to use a particular CDN server. Furthermore, the Netflix CDN uses push
caching rather than pull caching (Section 2.2.5): content is pushed into the servers at
scheduled times at off-peak hours, rather than dynamically during cache misses.

YouTube
With hundreds of hours of video uploaded to YouTube every minute and several
billion video views per day, YouTube is indisputably the world’s largest videosharing site. YouTube began its service in April 2005 and was acquired by Google
in November 2006. Although the Google/YouTube design and protocols are proprietary, through several independent measurement efforts we can gain a basic
understanding about how YouTube operates [Zink 2009; Torres 2011; Adhikari
2011a]. As with Netflix, YouTube makes extensive use of CDN technology to distribute its videos [Torres 2011]. Similar to Netflix, Google uses its own private CDN
to distribute YouTube videos, and has installed server clusters in many hundreds
of different IXP and ISP locations. From these locations and directly from its huge
data centers, Google distributes YouTube videos [Adhikari 2011a]. Unlike Netflix,
however, Google uses pull caching, as described in Section 2.2.5, and DNS redirect,
as described in Section 2.6.3. Most of the time, Google’s cluster-selection strategy
directs the client to the cluster for which the RTT between client and cluster is the
lowest; however, in order to balance the load across clusters, sometimes the client is
directed (via DNS) to a more distant cluster [Torres 2011].
YouTube employs HTTP streaming, often making a small number of different versions available for a video, each with a different bit rate and corresponding
quality level. YouTube does not employ adaptive streaming (such as DASH), but
instead requires the user to manually select a version. In order to save bandwidth
and server resources that would be wasted by repositioning or early termination,
YouTube uses the HTTP byte range request to limit the flow of transmitted data after
a target amount of video is prefetched.
Several million videos are uploaded to YouTube every day. Not only are YouTube videos streamed from server to client over HTTP, but YouTube uploaders also
upload their videos from client to server over HTTP. YouTube processes each video
it receives, converting it to a YouTube video format and creating multiple versions
at different bit rates. This processing takes place entirely within Google data centers.
(See the case study on Google’s network infrastructure in Section 2.6.3.)

2.7 Socket Programming: Creating Network
Applications

Now that we’ve looked at a number of important network applications, let’s explore
how network application programs are actually created. Recall from Section 2.1 that
a typical network application consists of a pair of programs—a client program and

