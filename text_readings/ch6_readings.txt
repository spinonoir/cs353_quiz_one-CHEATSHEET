6.6

•

DATA CENTER NETWORKING

6.6 Data Center Networking
Internet companies such as Google, Microsoft, Amazon, and Alibaba have built massive data centers, each housing tens to hundreds of thousands of hosts. As briefly
discussed in the sidebar in Section 1.2, data centers are not only connected to the
Internet, but also internally include complex computer networks, called data center
networks, which interconnect their internal hosts. In this section, we provide a brief
introduction to data center networking for cloud applications.
Broadly speaking, data centers serve three purposes. First, they provide
content such as Web pages, search results, e-mail, or streaming video to users.
Second, they serve as massively-parallel computing infrastructures for specific
data processing tasks, such as distributed index computations for search engines.
Third, they provide cloud computing to other companies. Indeed, today a major
trend in computing is for companies to use a cloud provider such as Amazon
Web Services, Microsoft Azure, and Alibaba Cloud to handle essentially all of
their IT needs.

6.6.1 Data Center Architectures
Data center designs are carefully kept company secrets, as they often provide critical
competitive advantages to leading cloud computing companies. The cost of a large
data center is huge, exceeding $12 million per month for a 100,000 host data center
in 2009 [Greenberg 2009a]. Of these costs, about 45 percent can be attributed to the
hosts themselves (which need to be replaced every 3–4 years); 25 percent to infrastructure, including transformers, uninterruptable power supplies (UPS) systems,
generators for long-term outages, and cooling systems; 15 percent for electric utility
costs for the power draw; and 15 percent for networking, including network gear
(switches, routers, and load balancers), external links, and transit traffic costs. (In
these percentages, costs for equipment are amortized so that a common cost metric
is applied for one-time purchases and ongoing expenses such as power.) While networking is not the largest cost, networking innovation is the key to reducing overall
cost and maximizing performance [Greenberg 2009a].
The worker bees in a data center are the hosts. The hosts in data centers, called
blades and resembling pizza boxes, are generally commodity hosts that include
CPU, memory, and disk storage. The hosts are stacked in racks, with each rack typically having 20 to 40 blades. At the top of each rack, there is a switch, aptly named
the Top of Rack (TOR) switch, that interconnects the hosts in the rack with each
other and with other switches in the data center. Specifically, each host in the rack
has a network interface that connects to its TOR switch, and each TOR switch has
additional ports that can be connected to other switches. Today, hosts typically have
40 Gbps or 100 Gbps Ethernet connections to their TOR switches [FB 2019; Greenberg 2015; Roy 2015; Singh 2015]. Each host is also assigned its own data-centerinternal IP address.

505

506

CHAPTER 6

•

THE LINK LAYER AND LANS

Internet
Border router

Access router
Load
balancer
B

Tier-1 switches

A

C

Tier-2 switches
TOR switches
Server racks

1

2

3

4

5

6

7

8

Figure 6.30 ♦ A data center network with a hierarchical topology
The data center network supports two types of traffic: traffic flowing between
external clients and internal hosts and traffic flowing between internal hosts. To
handle flows between external clients and internal hosts, the data center network
includes one or more border routers, connecting the data center network to the
public Internet. The data center network therefore interconnects the racks with each
other and connects the racks to the border routers. Figure 6.30 shows an example of a
data center network. Data center network design, the art of designing the interconnection network and protocols that connect the racks with each other and with the
border routers, has become an important branch of computer networking research in
recent years. (See references in this section.)

Load Balancing
A cloud data center, such as one operated by Google, Microsoft, Amazon, and Alibaba, provides many applications concurrently, such as search, e-mail, and video
applications. To support requests from external clients, each application is associated with a publicly visible IP address to which clients send their requests and from
which they receive responses. Inside the data center, the external requests are first

6.6

•

DATA CENTER NETWORKING

directed to a load balancer whose job it is to distribute requests to the hosts, balancing the load across the hosts as a function of their current load [Patel 2013; Eisenbud
2016]. A large data center will often have several load balancers, each one devoted
to a set of specific cloud applications. Such a load balancer is sometimes referred to
as a “layer-4 switch” since it makes decisions based on the destination port number
(layer 4) as well as destination IP address in the packet. Upon receiving a request for
a particular application, the load balancer forwards it to one of the hosts that handles
the application. (A host may then invoke the services of other hosts to help process
the request.) The load balancer not only balances the work load across hosts, but also
provides a NAT-like function, translating the public external IP address to the internal IP address of the appropriate host, and then translating back for packets traveling
in the reverse direction back to the clients. This prevents clients from contacting
hosts directly, which has the security benefit of hiding the internal network structure
and preventing clients from directly interacting with the hosts.

Hierarchical Architecture
For a small data center housing only a few thousand hosts, a simple network consisting of a border router, a load balancer, and a few tens of racks all interconnected by
a single Ethernet switch could possibly suffice. But to scale to tens to hundreds of
thousands of hosts, a data center often employs a hierarchy of routers and switches,
such as the topology shown in Figure 6.30. At the top of the hierarchy, the border
router connects to access routers (only two are shown in Figure 6.30, but there can be
many more). Below each access router, there are three tiers of switches. Each access
router connects to a top-tier switch, and each top-tier switch connects to multiple
second-tier switches and a load balancer. Each second-tier switch in turn connects to
multiple racks via the racks’ TOR switches (third-tier switches). All links typically
use Ethernet for their link-layer and physical-layer protocols, with a mix of copper
and fiber cabling. With such a hierarchical design, it is possible to scale a data center
to hundreds of thousands of hosts.
Because it is critical for a cloud application provider to continually provide applications with high availability, data centers also include redundant network equipment and redundant links in their designs (not shown in Figure 6.30). For example,
each TOR switch can connect to two tier-2 switches, and each access router, tier-1
switch, and tier-2 switch can be duplicated and integrated into the design [Cisco
2012; Greenberg 2009b]. In the hierarchical design in Figure 6.30, observe that the
hosts below each access router form a single subnet. In order to localize ARP broadcast traffic, each of these subnets is further partitioned into smaller VLAN subnets,
each comprising a few hundred hosts [Greenberg 2009a].
Although the conventional hierarchical architecture just described solves the
problem of scale, it suffers from limited host-to-host capacity [Greenberg 2009b].
To understand this limitation, consider again Figure 6.30, and suppose each host
connects to its TOR switch with a 10 Gbps link, whereas the links between switches

507

508

CHAPTER 6

•

THE LINK LAYER AND LANS

are 100 Gbps Ethernet links. Two hosts in the same rack can always communicate
at a full 10 Gbps, limited only by the rate of the hosts’ network interface controllers.
However, if there are many simultaneous flows in the data center network, the maximum rate between two hosts in different racks can be much less. To gain insight into
this issue, consider a traffic pattern consisting of 40 simultaneous flows between
40!pairs of hosts in different racks. Specifically, suppose each of 10 hosts in rack 1 in
Figure 6.30 sends a flow to a corresponding host in rack 5. Similarly, there are ten
simultaneous flows between pairs of hosts in racks 2 and 6, ten simultaneous flows
between racks 3 and 7, and ten simultaneous flows between racks 4 and 8. If each
flow evenly shares a link’s capacity with other flows traversing that link, then the
40 flows crossing the 100 Gbps A-to-B link (as well as the 100 Gbps B-to-C link)
will each only receive 100 Gbps / 40 = 2.5 Gbps, which is significantly less than the
10 Gbps network interface rate. The problem becomes even more acute for flows
between hosts that need to travel higher up the hierarchy.
There are several possible solutions to this problem:
• One possible solution to this limitation is to deploy higher-rate switches and
routers. But this would significantly increase the cost of the data center, because
switches and routers with high port speeds are very expensive.
• A second solution to this problem, which can be adopted whenever possible, is
to co-locate related services and data as close to one another as possible (e.g., in
the same rack or in a nearby rack) [Roy 2015; Singh 2015] in order to minimize
inter-rack communication via tier-2 or tier-1 switches. But this can only go so far,
as a key requirement in data centers is flexibility in placement of computation and
services [Greenberg 2009b; Farrington 2010]. For example, a large-scale Internet
search engine may run on thousands of hosts spread across multiple racks with
significant bandwidth requirements between all pairs of hosts. Similarly, a cloud
computing service (such Amazon Web Services or Microsoft Azure) may wish to
place the multiple virtual machines comprising a customer’s service on the physical hosts with the most capacity irrespective of their location in the data center.
If these physical hosts are spread across multiple racks, network bottlenecks as
described above may result in poor performance.
• A final piece of the solution is to provide increased connectivity between the TOR
switches and tier-2 switches, and between tier-2 switches and tier-1 switches. For
example, as shown in Figure 6.31, each TOR switch could be connected to two
tier-2 switches, which then provide for multiple link- and switch-disjoint paths
between racks. In Figure 6.31, there are four distinct paths between the first tier-2
switch and the second tier-2 switch, together providing an aggregate capacity of
400 Gbps between the first two tier-2 switches. Increasing the degree of connectivity between tiers has two significant benefits: there is both increased capacity and
increased reliability (because of path diversity) between switches. In Facebook’s
data center [FB 2014; FB 2019], each TOR is connected to four different tier-2
switches, and each tier-2 switch is connected to four different tier-1 switches.

6.6

•

DATA CENTER NETWORKING

509

Tier-1 switches

Tier-2 switches
TOR switches
Server racks
1

2

3

4

5

6

7

8

9

10

11

12

13

14

Figure 6.31 ♦ Highly interconnected data network topology
A direct consequence of the increased connectivity between tiers in data center
networks is that multi-path routing can become a first-class citizen in these networks. Flows are by default multipath flows. A very simple scheme to achieve
multi-path routing is Equal Cost Multi Path (ECMP) [RFC 2992], which performs a randomized next-hop selection along the switches between source and
destination. Advanced schemes using finer-grained load balancing have also been
proposed [Alizadeh 2014; Noormohammadpour 2018]. While these schemes perform multi-path routing at the flow level, there are also designs that route individual packets within a flow among multiple paths [He 2015; Raiciu 2010].

6.6.2 Trends in Data Center Networking
Data center networking is evolving rapidly, with the trends being driven by cost
reduction, virtualization, physical constraints, modularity, and customization.

Cost Reduction
In order to reduce the cost of data centers, and at the same time improve their delay
and throughput performance, as well as ease of expansion and deployment, Internet
cloud giants are continually deploying new data center network designs. Although
some of these designs are proprietary, others (e.g., [FB 2019]) are explicitly open or
described in the open literature (e.g., [Greenberg 2009b; Singh 2015]). Many important trends can thus be identified.
Figure 6.31 illustrates one of the most important trends in data center networking—the emergence of a hierarchical, tiered network interconnecting the data center
hosts. This hierarchy conceptually serves the same purpose as a single (very, very!),
large crossbar switch that we studied in Section 4.2.2, allowing any host in the
data center to communicate with any other host. But as we have seen, this tiered

15

16

510

CHAPTER 6

•

THE LINK LAYER AND LANS

interconnection network has many advantages over a conceptual crossbar switch,
including multiple paths from source to destination and the increased capacity (due
to multipath routing) and reliability (due to multiple switch- and link-disjoint paths
between any two hosts).
The data center interconnection network is comprised of a large number of smallsized switches. For example, in Google’s Jupiter datacenter fabric, one configuration has
48 links between the ToR switch and its servers below, and connections up to 8 tier-2
switches; a tier-2 switch has links to 256 ToR switches and links up to 16 tier-1 switches
[Singh 2015]. In Facebook’s data center architecture, each ToR switch connects up to four
different tier-2 switches (each in a different “spline plane”), and each tier-2 switch connects
up to 4 of the 48 tier-1 switches in its spline plane; there are four spline planes. Tier-1 and
tier-2 switches connect down to a larger, scalable number of tier-2 or ToR switches, respectively, below [FB 2019]. For some of the largest data center operators, these switches
are being built in-house from commodity, off-the-shelf, merchant silicon [Greenberg
2009b; Roy 2015; Singh 2015] rather than being purchased from switch vendors.
A multi-switch layered (tiered, multistage) interconnection network such as that
in Figure 6.31 and as implemented in the data center architectures discussed above
is known as Clos networks, named after Charles Clos, who studied such networks
[Clos 1953] in the context of telephony switching. Since then, a rich theory of Clos
networks has been developed, finding additional use in data center networking and
in multiprocessor interconnection networks.

Centralized SDN Control and Management
Because a data center is managed by a single organization, it is perhaps natural that
a number of the largest data center operators, including Google, Microsoft, and
Facebook, are embracing the notion of SDN-like logically centralized control. Their
architectures also reflect a clear separation of a data plane (comprised of relatively
simple, commodity switches) and a software-based control plane, as we saw in Section 5.5. Due to the immense-scale of their data centers, automated configuration and
operational state management, as we encountered in Section 5.7, are also crucial.

Virtualization
Virtualization has been a driving force for much of the growth of cloud computing
and data center networks more generally. Virtual Machines (VMs) decouple software running applications from the physical hardware. This decoupling also allows
seamless migration of VMs between physical servers, which might be located on
different racks. Standard Ethernet and IP protocols have limitations in enabling the
movement of VMs while maintaining active network connections across servers.
Since all data center networks are managed by a single administrative authority, an
elegant solution to the problem is to treat the entire data center network as a single,
flat, layer-2 network. Recall that in a typical Ethernet network, the ARP protocol
maintains the binding between the IP address and hardware (MAC) address on an

6.6

•

DATA CENTER NETWORKING

interface. To emulate the effect of having all hosts connect to a “single” switch, the
ARP mechanism is modified to use a DNS style query system instead of a broadcast,
and the directory maintains a mapping of the IP address assigned to a VM and which
physical switch the VM is currently connected to in the data center network. Scalable schemes that implement this basic design have been proposed in [Mysore 2009;
Greenberg 2009b] and have been successfully deployed in modern data centers.

Physical Constraints
Unlike the wide area Internet, data center networks operate in environments that not
only have very high capacity (40 Gbps and 100 Gbps links are now commonplace)
but also have extremely low delays (microseconds). Consequently, buffer sizes are
small and congestion control protocols such as TCP and its variants do not scale
well in data centers. In data centers, congestion control protocols have to react fast
and operate in extremely low loss regimes, as loss recovery and timeouts can lead
to extreme inefficiency. Several approaches to tackle this issue have been proposed
and deployed, ranging from data center-specific TCP variants [Alizadeh 2010] to
implementing Remote Direct Memory Access (RDMA) technologies on standard
Ethernet [Zhu 2015; Moshref 2016; Guo 2016]. Scheduling theory has also been
applied to develop mechanisms that decouple flow scheduling from rate control,
enabling very simple congestion control protocols while maintaining high utilization
of the links [Alizadeh 2013; Hong 2012].

Hardware Modularity and Customization
Another major trend is to employ shipping container–based modular data centers
(MDCs) [YouTube 2009; Waldrop 2007]. In an MDC, a factory builds, within a
standard 12-meter shipping container, a “mini data center” and ships the container to
the data center location. Each container has up to a few thousand hosts, stacked in tens
of racks, which are packed closely together. At the data center location, multiple containers are interconnected with each other and also with the Internet. Once a prefabricated container is deployed at a data center, it is often difficult to service. Thus, each
container is designed for graceful performance degradation: as components (servers
and switches) fail over time, the container continues to operate but with degraded performance. When many components have failed and performance has dropped below a
threshold, the entire container is removed and replaced with a fresh one.
Building a data center out of containers creates new networking challenges.
With an MDC, there are two types of networks: the container-internal networks
within each of the containers and the core network connecting each container [Guo
2009; Farrington 2010]. Within each container, at the scale of up to a few thousand
hosts, it is possible to build a fully connected network using inexpensive commodity
Gigabit Ethernet switches. However, the design of the core network, interconnecting
hundreds to thousands of containers while providing high host-to-host bandwidth
across containers for typical workloads, remains a challenging problem. A hybrid

511

512

CHAPTER 6

•

THE LINK LAYER AND LANS

electrical/optical switch architecture for interconnecting the containers is described
in [Farrington 2010].
Another important trend is that large cloud providers are increasingly building
or customizing just about everything that is in their data centers, including network
adapters, switches routers, TORs, software, and networking protocols [Greenberg
2015; Singh 2015]. Another trend, pioneered by Amazon, is to improve reliability
with “availability zones,” which essentially replicate distinct data centers in different
nearby buildings. By having the buildings nearby (a few kilometers apart), transactional data can be synchronized across the data centers in the same availability
zone while providing fault tolerance [Amazon 2014]. Many more innovations in data
center design are likely to continue to come.

6.7 Retrospective: A Day in the Life of a Web
Page Request

Now that we’ve covered the link layer in this chapter, and the network, transport and
application layers in earlier chapters, our journey down the protocol stack is complete! In the very beginning of this book (Section 1.1), we wrote “much of this book
is concerned with computer network protocols,” and in the first five chapters, we’ve
certainly seen that this is indeed the case! Before heading into the topical chapters in
second part of this book, we’d like to wrap up our journey down the protocol stack by
taking an integrated, holistic view of the protocols we’ve learned about so far. One
way then to take this “big picture” view is to identify the many (many!) protocols
that are involved in satisfying even the simplest request: downloading a Web page.
Figure 6.32 illustrates our setting: a student, Bob, connects a laptop to his school’s
Ethernet switch and downloads a Web page (say the home page of www.google.com).
As we now know, there’s a lot going on “under the hood” to satisfy this seemingly
simple request. A Wireshark lab at the end of this chapter examines trace files containing a number of the packets involved in similar scenarios in more detail.

6.7.1 Getting Started: DHCP, UDP, IP, and Ethernet
Let’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable
connected to the school’s Ethernet switch, which in turn is connected to the school’s
router, as shown in Figure 6.32. The school’s router is connected to an ISP, in this
example, comcast.net. In this example, comcast.net is providing the DNS service
for the school; thus, the DNS server resides in the Comcast network rather than the
school network. We’ll assume that the DHCP server is running within the router, as
is often the case.
When Bob first connects his laptop to the network, he can’t do anything
(e.g., download a Web page) without an IP address. Thus, the first network-related

